"""
Prompt factory for generating LLM prompts for different endpoints.

This module provides functions to generate contextual prompts for:
- AI likelihood detection
- Essay feedback generation
- Section guidance
- Section checking
"""

from typing import List, Optional
from app.models.types import Section
from app.core.config import settings
from app.prompts.criteria_data import ESSAY_RUBRIC_CRITERIA, DEFAULT_CRITERIA


# Educational rule to prevent LLM from doing student's work
EDUCATIONAL_RULE = """
CRITICAL: You are an educational assistant helping students learn. 
- NEVER generate complete essays, sentences, or paragraphs for the student.
- Your role is to GUIDE and PROVIDE FEEDBACK, not to write their content.
- Suggest improvements and ask questions, but let the student write their own work.
"""


def generate_prompt_for_ai_detection(text: str, language: Optional[str] = None) -> str:
    """
    Generate prompt for AI likelihood detection.
    
    Args:
        text: The text to analyze
        language: Language for the response (en=English, es=Spanish), defaults to config
        
    Returns:
        Formatted prompt string
    """
    # Use default language from config if not provided
    if language is None:
        language = settings.DEFAULT_LANGUAGE
    
    lang_name = "Español" if language == "es" else "English"
    
    return f"""
{EDUCATIONAL_RULE}

You are an AI detection expert analyzing student essays to help educators identify potentially AI-generated content.

IMPORTANT: If the text below is in {lang_name}, you MUST respond ENTIRELY in {lang_name} language.

Your task is to analyze the following text and determine the likelihood that it was generated by an AI tool (like ChatGPT, Claude, etc.).

Guidelines:
- Consider factors like: formulaic phrasing, lack of personal voice, overly formal tone, lack of natural errors
- Score 0-30: Highly likely to be student-written (has personal voice, natural errors, authentic style)
- Score 31-60: Possibly mixed or uncertain (some AI-like patterns but also human-like characteristics)
- Score 61-85: Likely AI-generated (strong patterns of AI generation, formulaic structure)
- Score 86-100: Almost certainly AI-generated (clear AI patterns, no personal voice, formulaic)

Text to analyze:
{text}

Return ONLY valid JSON in this exact format:
{{
    "score": <integer between 0 and 100>,
    "rationale": "<brief explanation of your assessment, 2-3 sentences>",
    "caveats": ["<caveat 1>", "<caveat 2>", "<etc.>"]
}}

Important: Only return the JSON, no additional text before or after.
"""


def generate_prompt_for_feedback(text: str, criteria: Optional[List[str]] = None, language: Optional[str] = None) -> str:
    """
    Generate prompt for essay feedback generation.
    
    Args:
        text: The text to provide feedback on
        criteria: Optional list of specific criteria to evaluate
        language: Language for the response (en=English, es=Spanish), defaults to config
        
    Returns:
        Formatted prompt string
    """
    # Use default language from config if not provided
    if language is None:
        language = settings.DEFAULT_LANGUAGE
    
    # Use Extended Toulmin Model criteria
    used_criteria = criteria if criteria else DEFAULT_CRITERIA
    
    # Build criteria description
    criteria_descriptions = []
    for crit in used_criteria:
        crit_info = ESSAY_RUBRIC_CRITERIA.get(crit, {})
        desc = crit_info.get('description', '')
        max_pts = crit_info.get('maxPoints', 0)
        criteria_descriptions.append(
            f"- '{crit}': {desc} (valor máximo {max_pts} puntos)"
        )
    
    criteria_text = "\n      ".join(criteria_descriptions)
    
    lang_name = "Español" if language == "es" else "English"
    
    return f"""
{EDUCATIONAL_RULE}

You are an experienced high school teacher evaluating essays using the Extended Toulmin Model rubric.

IMPORTANT: The essay below is written in {lang_name}. You MUST respond ENTIRELY in {lang_name} language.

Your task is to evaluate the following essay using this rubric:

      {criteria_text}

Here is the essay to evaluate:
"{text}"

Analyze the essay and evaluate it according to each criterion. For each criterion, you must provide:
1. etiqueta: The criterion name in lowercase (e.g., "originalidad")
2. criterio: The full description of the criterion
3. valorMaximo: The maximum possible points for this criterion
4. logro: Achievement level ("Excepcional", "Muy Bien", "Bien", "Regular", "Insuficiente")
5. evaluacion: Qualitative description of the student's performance (2-3 sentences)
6. puntuacion: Numerical score (between 0 and valorMaximo) - MUST be consistent with logro level

SCORING GUIDELINES - You MUST follow these score ranges for each achievement level:
- Excepcional: Award 90-100% of valorMaximo points
- Muy Bien: Award 75-89% of valorMaximo points
- Bien: Award 60-74% of valorMaximo points
- Regular: Award 40-59% of valorMaximo points
- Insuficiente: Award 20-39% of valorMaximo points (DO NOT give 0 unless the work is completely absent or nonsense)

IMPORTANT: If the evaluacion mentions ANY positive aspects or partial work, the score MUST be at least 20% of valorMaximo. Only give 0 if there is absolutely no attempt or the text is nonsense.

Return ONLY valid JSON in this exact format:
{{
    "overview": "<Overall assessment of the essay in 2-3 sentences>",
    "per_criterion": [
        {{
            "etiqueta": "originalidad",
            "criterio": "¿Se usan enfoques creativos, metáforas o comparaciones inesperadas?",
            "valorMaximo": 22,
            "logro": "Muy Bien",
            "evaluacion": "Description of student performance...",
            "puntuacion": 17
        }},
        ...
    ]
}}

Important:
- Use a formal tone for all evaluations
- Be consistent in scoring (aim for 95% reproducibility)
- Match the puntuacion to the logro level using the scoring guidelines above
- Only set puntuacion to 0 if there is absolutely no attempt or the text is nonsense
- Return ONLY the JSON array, no additional text before or after
- All text MUST be in {lang_name} language
"""


def generate_prompt_for_guidance(section: Section, language: Optional[str] = None) -> str:
    """
    Generate prompt for essay section guidance.
    
    Args:
        section: The section to provide guidance for (claim, reasoning, etc.)
        language: Language for the response (en=English, es=Spanish), defaults to config
        
    Returns:
        Formatted prompt string
    """
    # Use default language from config if not provided
    if language is None:
        language = settings.DEFAULT_LANGUAGE
    
    lang_name = "Español" if language == "es" else "English"
    
    section_info = {
        "claim": {
            "description": "The main argument or thesis statement",
            "purpose": "Present the central argument that the essay will defend",
            "key_elements": ["clarity", "debatability", "specificity", "strength"]
        },
        "reasoning": {
            "description": "The logical justification for the claim",
            "purpose": "Explain why the claim is valid through logical reasoning",
            "key_elements": ["logical flow", "coherence", "connection to claim"]
        },
        "evidence": {
            "description": "Facts, data, examples that support the reasoning",
            "purpose": "Provide concrete support for the reasoning through facts and examples",
            "key_elements": ["relevance", "credibility", "specificity", "variety"]
        },
        "backing": {
            "description": "Additional support or authority that reinforces the claim",
            "purpose": "Strengthen the argument with additional context or authority",
            "key_elements": ["authority", "context", "reinforcement"]
        },
        "reservation": {
            "description": "Limitations or conditions that acknowledge counter-arguments",
            "purpose": "Show awareness of limitations and strengthen credibility",
            "key_elements": ["honesty", "self-awareness", "scope definition"]
        },
        "rebuttal": {
            "description": "Response to counter-arguments or objections",
            "purpose": "Address potential opposing viewpoints and strengthen the argument",
            "key_elements": ["anticipation", "response", "refutation"]
        }
    }
    
    info = section_info.get(section, section_info["claim"])
    
    return f"""
{EDUCATIONAL_RULE}

You are an expert writing tutor helping students understand how to write the "{section}" section of an argumentative essay based on the Toulmin model.

IMPORTANT: You MUST provide all guidance ENTIRELY in {lang_name} language.

Section: {section}
Description: {info["description"]}
Purpose: {info["purpose"]}

Your task is to provide educational guidance for writing this section.

Return ONLY valid JSON in this exact format:
{{
    "section": "{section}",
    "purpose": "{info["purpose"]}",
    "steps": ["<step 1>", "<step 2>", "<step 3>", "<step 4>"],
    "checklist": ["<item 1>", "<item 2>", "<item 3>", "<item 4>", "<item 5>"],
    "examples_do": ["<good example 1>", "<good example 2>"],
    "examples_dont": ["<bad example 1>", "<bad example 2>"],
    "tips": ["<tip 1>", "<tip 2>", "<tip 3>", "<tip 4>"]
}}

Guidelines for content:
- Keep ALL text SHORT (max 15 words per item)
- "steps": 3-4 specific steps
- "checklist": 4-5 key elements
- "examples_do": 2 examples (SHORT only)
- "examples_dont": 2 examples (SHORT only)
- "tips": 3-4 tips

CRITICAL: Return ONLY the JSON object. No text before or after. Keep responses concise.

IMPORTANT: Provide all text in {language.upper()} language.
"""


def generate_prompt_for_section_check(section: Section, text: str, language: Optional[str] = None) -> str:
    """
    Generate prompt for checking and improving a specific essay section.
    
    Args:
        section: The section being checked
        text: The student's text for this section
        language: Language for the feedback (en=English, es=Spanish), defaults to config
        
    Returns:
        Formatted prompt string
    """
    # Use default language from config if not provided
    if language is None:
        language = settings.DEFAULT_LANGUAGE
    
    lang_name = "Español" if language == "es" else "English"
    
    return f"""
{EDUCATIONAL_RULE}

You are an expert writing tutor reviewing a student's "{section}" section from their argumentative essay.

IMPORTANT: The student's text below is in {lang_name}. You MUST respond ENTIRELY in {lang_name} language.

Your task is to provide constructive, specific feedback to help the student improve.

Section: {section}

Student's text to review:
{text}

Provide feedback that:
- Highlights what the student did well (2-3 specific strengths)
- Identifies issues or areas for improvement (2-4 specific issues)
- Asks guiding questions to help the student think critically about their writing (3-4 questions)
- Suggests concrete revision strategies (3-4 actionable strategies)

Return ONLY valid JSON in this exact format:
{{
    "section": "{section}",
    "strengths": ["<strength 1>", "<strength 2>", "<strength 3>"],
    "issues": ["<issue 1>", "<issue 2>", "<issue 3>"],
    "questions_to_refine": ["<question 1>", "<question 2>", "<question 3>"],
    "revision_strategies": ["<strategy 1>", "<strategy 2>", "<strategy 3>"]
}}

Important: 
- Focus on specific, actionable feedback
- Use constructive language
- Ask questions that guide critical thinking
- Only return the JSON, no additional text before or after.

CRITICAL: Provide all feedback in {language.upper()} language.
"""

