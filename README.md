# ğŸ§  Essay Mentor API

> **FastAPI + LangChain + Modular Architecture + LLM Adapters with Advanced AI Features**

A production-ready FastAPI service designed to **analyze and guide** the writing of short student essays (max. 8,000 characters). Built with clean architecture principles, comprehensive testing, enterprise-grade features, and **advanced LangChain integration** for sophisticated AI workflows.

The service provides automated tools to:

* **Estimate** whether a text may have been generated by AI.
* Offer **personalized feedback** based on multiple criteria (originality, creativity, coherence, etc.).
* **Advanced analysis** using LangChain chains for detailed, multi-step evaluation.
* **Guide** students with section-specific explanations and writing tips for argumentative essays based on the **Extended Toulmin Model**:
    * *Originality* (22 points)
    * *Depth* (18 points) 
    * *Integrality* (16 points)
    * *Conciliation* (14 points)
    * *Refutation* (12 points)
    * *Evidence* (10 points)
    * *Logic* (8 points)

---

## âœ¨ Key Features

### Core Features
* **Modular architecture** with clear separation of concerns (routers, services, adapters, prompts, models, utils)
* **Interchangeable LLM adapters** (Ollama for local development, OpenAI for production, LangChain for advanced features)
* **Swagger-ready endpoints** with comprehensive API documentation
* **Configurable prompts** with educational rules (never generate the student's answer)
* **Structured feedback** based on customizable evaluation criteria
* **Bilingual-ready**: Configure default language via `DEFAULT_LANGUAGE` env variable (en/es)

### ğŸš€ Advanced LangChain Features
* **Multi-step analysis chains** for comprehensive essay evaluation
* **PydanticOutputParser** for robust, structured LLM responses
* **Context-aware processing** with accumulated knowledge across analysis steps
* **Granular criterion evaluation** with individual LangChain chains
* **Advanced error handling** with automatic JSON cleaning and validation
* **Memory integration** (ready for conversational analysis)
* **RAG capabilities** (ready for knowledge base integration)
* **Agent workflows** (ready for complex reasoning tasks)

### Enterprise Features
* **Token usage tracking** with SQLite database
* **Cost tracking** for OpenAI usage with daily/monthly reports
* **Intelligent fallback system** for OpenAI models (gpt-4o â†’ gpt-4o-mini â†’ gpt-3.5-turbo)
* **Retry logic** with exponential backoff and jitter
* **Circuit breakers** to prevent cascading failures
* **Function-specific model selection** for cost optimization
* **Usage analytics** API with CSV export capability

---

## ğŸ—ï¸ Project Structure

```
EssayMentorApi/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                           # FastAPI application entry point
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ config.py                     # Configuration management
â”‚   â”œâ”€â”€ adapters/
â”‚   â”‚   â”œâ”€â”€ base.py                       # Base LLM adapter interface
â”‚   â”‚   â”œâ”€â”€ registry.py                   # Provider registry system
â”‚   â”‚   â”œâ”€â”€ llm_adapter.py                # Unified LLM adapter wrapper
â”‚   â”‚   â””â”€â”€ providers/                    # Provider-specific adapters
â”‚   â”‚       â”œâ”€â”€ ollama.py                 # Ollama adapter (local models)
â”‚   â”‚       â”œâ”€â”€ openai.py                 # OpenAI adapter (API models)
â”‚   â”‚       â””â”€â”€ langchain_adapter.py      # LangChain adapter (advanced features)
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ types.py                      # Section enums and mappings
â”‚   â”‚   â”œâ”€â”€ essay.py                      # Essay request/response models
â”‚   â”‚   â”œâ”€â”€ analyze.py                    # AI detection & feedback models
â”‚   â”‚   â”œâ”€â”€ guide.py                      # Guidance models
â”‚   â”‚   â”œâ”€â”€ criterion.py                  # Criterion evaluation models
â”‚   â”‚   â””â”€â”€ usage.py                      # Token usage tracking models
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â”œâ”€â”€ factory.py                    # Prompt generation wrapper
â”‚   â”‚   â”œâ”€â”€ criteria_data.py              # Extended Toulmin Model criteria
â”‚   â”‚   â””â”€â”€ rubric_criteria.py            # Rubric definitions
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ analyzer.py                   # AI detection & feedback logic
â”‚   â”‚   â”œâ”€â”€ guidance.py                   # Essay section guidance logic
â”‚   â”‚   â””â”€â”€ advanced_analyzer.py          # LangChain-powered advanced analysis
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ meta.py                       # Health & metadata endpoints
â”‚   â”‚   â”œâ”€â”€ analyze.py                    # Analysis endpoints
â”‚   â”‚   â”œâ”€â”€ guide.py                      # Guidance endpoints
â”‚   â”‚   â”œâ”€â”€ advanced.py                   # Advanced LangChain endpoints
â”‚   â”‚   â””â”€â”€ usage.py                      # Token usage & reporting endpoints
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ json_parse.py                 # Robust JSON parsing
â”‚       â”œâ”€â”€ criteria.py                   # Criteria validation
â”‚       â”œâ”€â”€ text_format.py                # Text formatting utilities
â”‚       â”œâ”€â”€ token_tracker.py              # Token tracking wrapper
â”‚       â”œâ”€â”€ fallback_manager.py           # Fallback system wrapper
â”‚       â”œâ”€â”€ tracking/                     # Token tracking components
â”‚       â”‚   â”œâ”€â”€ database_manager.py      # SQLite operations
â”‚       â”‚   â”œâ”€â”€ cost_calculator.py       # Cost calculations
â”‚       â”‚   â”œâ”€â”€ report_generator.py      # Usage reports
â”‚       â”‚   â””â”€â”€ __init__.py              # Main tracker class
â”‚       â””â”€â”€ fallback/                     # Fallback system components
â”‚           â”œâ”€â”€ circuit_breaker.py       # Circuit breaker logic
â”‚           â”œâ”€â”€ retry_logic.py           # Retry strategies
â”‚           â”œâ”€â”€ error_classifier.py      # Error classification
â”‚           â””â”€â”€ __init__.py              # Main fallback manager
â”œâ”€â”€ tests/                                # Comprehensive test suite
â”‚   â”œâ”€â”€ conftest.py                      # Shared fixtures
â”‚   â”œâ”€â”€ test_adapters_llm.py            # Adapter tests
â”‚   â”œâ”€â”€ test_services_analyzer.py       # Analyzer tests
â”‚   â”œâ”€â”€ test_services_guidance.py       # Guidance tests
â”‚   â”œâ”€â”€ test_services_advanced_analyzer.py # Advanced analyzer tests
â”‚   â”œâ”€â”€ test_utils_json_parse.py        # Utils tests
â”‚   â”œâ”€â”€ test_utils_criteria.py          # Criteria tests
â”‚   â”œâ”€â”€ test_utils_text_format.py       # Text format tests
â”‚   â””â”€â”€ test_routers_integration.py     # Integration tests
â”œâ”€â”€ examples/                             # Usage examples
â”œâ”€â”€ requirements.txt                      # Python dependencies
â”œâ”€â”€ .env.example                          # Environment variables template
â””â”€â”€ README.md                             # This file
```

---

## ğŸš€ Quick Start

### 1. Clone the repository

```bash
git clone https://github.com/<your-username>/EssayMentorApi.git
cd EssayMentorApi
```

### 2. Create virtual environment

```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Configure environment variables

Create a `.env` file based on `.env.example`:

```bash
# Application Configuration
APP_NAME=Essay Mentor API
APP_VERSION=1.0.0
HOST=0.0.0.0
PORT=8000
CORS_ORIGINS=*

# LLM Configuration
LLM_PROVIDER=langchain  # or 'ollama', 'openai', 'qwen2.5'
LLM_MODEL=llama3.1      # Default model
OLLAMA_URL=http://localhost:11434

# OpenAI Configuration (for production)
OPENAI_API_KEY=your_key_here
OPENAI_BASE_URL=https://api.openai.com/v1

# Function-specific model selection (cost optimization)
LLM_MODEL_AI_DETECTION=   # Empty = use LLM_MODEL
LLM_MODEL_FEEDBACK=        # Empty = use LLM_MODEL
LLM_MODEL_GUIDANCE=        # Empty = use LLM_MODEL
LLM_MODEL_SECTION_CHECK=   # Empty = use LLM_MODEL

# Fallback Configuration (OpenAI models only)
LLM_FALLBACK_AI_DETECTION=gpt-4o,gpt-4o-mini,gpt-3.5-turbo
LLM_FALLBACK_FEEDBACK=gpt-4o,gpt-4o-mini,gpt-3.5-turbo
LLM_FALLBACK_GUIDANCE=gpt-4o-mini,gpt-3.5-turbo
LLM_FALLBACK_SECTION_CHECK=gpt-4o-mini,gpt-3.5-turbo

# Retry Configuration
GPT4O_MAX_RETRIES=2
GPT4O_MINI_MAX_RETRIES=3
GPT35_TURBO_MAX_RETRIES=5

# Circuit Breaker Configuration
CIRCUIT_BREAKER_FAILURE_THRESHOLD=5
CIRCUIT_BREAKER_RECOVERY_TIMEOUT=300

# Language Configuration
DEFAULT_LANGUAGE=en  # en=English, es=Spanish

# Token Tracking Configuration
ENABLE_TOKEN_TRACKING=true
TOKEN_TRACKING_DB_PATH=usage_tracking.db
```

### 5. Run the API

```bash
# Development
uvicorn app.main:app --reload

# Production
gunicorn app.main:app -w 4 -k uvicorn.workers.UvicornWorker
```

**Access Points:**
* **Swagger UI:** `http://localhost:8000/docs`
* **ReDoc:** `http://localhost:8000/redoc`
* **OpenAPI JSON:** `http://localhost:8000/openapi.json`

---

## ğŸ“š API Endpoints

### Analysis Endpoints

| Method | Route | Description |
|:---|:---|:---|
| `POST` | `/analyze/ai-likelihood` | Estimate AI-generated likelihood (0-100 score) |
| `POST` | `/analyze/feedback` | Generate structured writing feedback with criteria evaluation |

### ğŸš€ Advanced LangChain Endpoints

| Method | Route | Description |
|:---|:---|:---|
| `POST` | `/advanced/analyze-chain` | **Advanced multi-step analysis** using LangChain chains with Extended Toulmin Model |
| `POST` | `/advanced/feedback-chains` | **Granular criterion analysis** with individual LangChain chains per criterion |

### Guidance Endpoints

| Method | Route | Description |
|:---|:---|:---|
| `POST` | `/guide` | Provide guidance for essay sections (claim, reasoning, etc.) |
| `POST` | `/guide/check-section` | Review a specific section and suggest improvements |

### Usage & Reporting Endpoints

| Method | Route | Description |
|:---|:---|:---|
| `GET` | `/admin/usage/daily` | Get daily token usage report |
| `GET` | `/admin/usage/report` | Get usage report for date range |
| `GET` | `/admin/usage/costs` | Get cost summary |
| `GET` | `/admin/usage/trends` | Get usage trends |
| `GET` | `/admin/usage/export/csv` | Export usage data as CSV |

### Meta Endpoints

| Method | Route | Description |
|:---|:---|:---|
| `GET` | `/health` | Check API health and LLM provider status |

---

## ğŸ“– Example Usage

### ğŸš€ Advanced Analysis with LangChain

```bash
curl -X POST "http://localhost:8000/advanced/analyze-chain" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "La inteligencia artificial estÃ¡ transformando nuestra sociedad de manera significativa. Este ensayo analiza tres aspectos fundamentales de esta revoluciÃ³n tecnolÃ³gica...",
    "criteria": ["originalidad", "profundidad", "evidencia"],
    "language": "es"
  }'
```

**Response:**
```json
{
  "overview": "La evaluaciÃ³n general del ensayo es positiva. El autor aborda tres aspectos cruciales de la transformaciÃ³n social impulsada por la inteligencia artificial...",
  "per_criterion": [
    {
      "etiqueta": "originalidad",
      "criterio": "Â¿Se usan enfoques creativos, metÃ¡foras o comparaciones inesperadas?",
      "valorMaximo": 22,
      "logro": "El ensayo utiliza una variedad de mÃ©todos creativos y no convencionales para explicar los impactos de la IA...",
      "evaluacion": "bueno",
      "puntuacion": 15
    }
  ],
  "total_score": 66,
  "max_possible_score": 100,
  "percentage": 66.0
}
```

### Standard Analysis

```bash
curl -X POST "http://localhost:8000/analyze/feedback" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Students should learn critical thinking...",
    "criteria": ["originalidad", "coherencia"],
    "language": "es"
  }'
```

### Get Section Guidance

```bash
curl -X POST "http://localhost:8000/guide" \
  -H "Content-Type: application/json" \
  -d '{
    "section": "claim",
    "language": "en"
  }'
```

---

## ğŸ” LangChain vs Standard Analysis Comparison

### **Quality Comparison**

| Aspect | Standard Analysis | Advanced LangChain Analysis |
|--------|------------------|----------------------------|
| **Overview Length** | 1 sentence (25 words) | 4 paragraphs (150+ words) |
| **Detail Level** | Superficial | Deep and structured |
| **Strengths Identification** | Not mentioned | Specific strengths identified |
| **Improvement Suggestions** | Not mentioned | Concrete improvement suggestions |
| **Processing Method** | 1 LLM call | 8 LLM calls (1 overview + 7 criteria) |
| **Context Awareness** | Limited | Contextual accumulation |
| **Precision** | General | Specific per criterion |
| **Consistency** | Variable | High (Pydantic validation) |

### **Technical Benefits**

| Feature | Standard | LangChain Advanced |
|---------|----------|-------------------|
| **JSON Parsing** | Manual `safe_json_parse()` (227 lines) | `PydanticOutputParser` (automatic) |
| **Error Handling** | Manual JSON error handling | Automatic markdown cleaning + validation |
| **Type Safety** | Basic Pydantic validation | Advanced Pydantic with auto-conversion |
| **Extensibility** | Modify complex prompts | Add criteria to simple loop |
| **Debugging** | Difficult (JSON parsing issues) | Easy (structured logs) |
| **Testing** | Complex (manual parsing) | Simple (Pydantic validation) |

---

## âš™ï¸ LLM Provider Configuration

### Registry Pattern Architecture

The project uses a **Registry Pattern** for LLM providers, making it extremely easy to add new providers without modifying existing code.

#### Supported Providers

| Provider | Description | Setup | Adapter |
|:---------|:------------|:------|:--------|
| `ollama` | Local Ollama models (Llama, Mistral, etc.) | `ollama pull llama3.1 && ollama serve` | OllamaAdapter |
| `openai` | OpenAI API models (GPT-4, GPT-3.5) | Set `OPENAI_API_KEY` environment variable | OpenAIAdapter |
| `qwen2.5` | Qwen2.5 models via Ollama | `ollama pull qwen2.5 && ollama serve` | OllamaAdapter |
| `langchain` | **Advanced LangChain features** | Uses OpenAI/Ollama with LangChain | LangChainAdapter |

### ğŸš€ Adding New LLM Providers

Adding a new LLM provider is now **incredibly simple** - just 2 steps:

#### Step 1: Create Provider Adapter
Create `app/adapters/providers/your_provider.py`:

```python
from app.adapters.base import BaseLLMAdapter

class YourProviderAdapter(BaseLLMAdapter):
    def generate(self, prompt: str, **kwargs) -> str:
        # Your provider-specific implementation
        # Handle API calls, authentication, response parsing
        return response_text
```

#### Step 2: Register the Provider
Add 2 lines to `app/adapters/llm_adapter.py`:

```python
from app.adapters.providers.your_provider import YourProviderAdapter
LLMProviderRegistry.register("your_provider", YourProviderAdapter)
```

#### Step 3: Use It! ğŸ‰
```bash
LLM_PROVIDER=your_provider
LLM_MODEL=your_model_name
```

**That's it!** No other files need to be modified.

### Configuration Examples

#### Development with Ollama
```bash
LLM_PROVIDER=ollama
LLM_MODEL=llama3.1
```

#### Development with Qwen2.5
```bash
LLM_PROVIDER=qwen2.5
LLM_MODEL=qwen2.5:3b
```

#### Production with OpenAI
```bash
LLM_PROVIDER=openai
LLM_MODEL=gpt-4o-mini
OPENAI_API_KEY=your_api_key_here
```

#### ğŸš€ Advanced Analysis with LangChain
```bash
LLM_PROVIDER=langchain
LLM_MODEL=gpt-4o-mini  # or llama3.1 for local
OPENAI_API_KEY=your_api_key_here  # if using OpenAI models
```

#### Mixed Configuration (Cost Optimization)
```bash
LLM_PROVIDER=langchain
LLM_MODEL=gpt-4o-mini
LLM_MODEL_AI_DETECTION=openai:gpt-4o          # Premium model for critical tasks
LLM_MODEL_FEEDBACK=openai:gpt-4o-mini         # Balanced model for feedback
LLM_MODEL_GUIDANCE=qwen2.5:qwen2.5:3b         # Free model for guidance
LLM_MODEL_SECTION_CHECK=qwen2.5:qwen2.5:3b    # Free model for section checks
```

---

## ğŸ§ª Testing

The project includes comprehensive test coverage with 107+ tests.

### Run Tests

```bash
# All tests
pytest

# With verbose output
pytest -v

# With coverage report
pytest --cov=app --cov-report=html

# Specific test file
pytest tests/test_services_advanced_analyzer.py

# LangChain-specific tests
pytest tests/test_services_advanced_analyzer.py::test_analyze_with_chain
```

### Test Structure

- **Unit Tests**: Services, adapters, utilities
- **Integration Tests**: API endpoints
- **LangChain Tests**: Advanced analyzer functionality
- **Fixtures**: Mock LLM responses
- **Coverage**: ~90% code coverage

**Current Status:** âœ… 107+ tests passing

---

## ğŸ¯ Architecture Highlights

### Clean Architecture
- **Routers** â†’ **Services** â†’ **Adapters** â†’ **Models**
- Clear separation of concerns
- No circular dependencies
- Testable components

### ğŸš€ Advanced LangChain Integration
- **Multi-step Chains**: Sequential analysis with context accumulation
- **PydanticOutputParser**: Robust structured output validation
- **Context Awareness**: Each step builds upon previous analysis
- **Granular Processing**: Individual chains per evaluation criterion
- **Error Resilience**: Automatic JSON cleaning and validation

### Modular Prompt Generation
- Separated concerns (constants, languages, sections, templates)
- Reusable components
- Easy to extend and maintain

### Robust LLM Integration
- **Registry Pattern**: Easy to add new LLM providers
- **Unified Interface**: Single API for all providers
- **Multi-Provider Support**: Ollama, OpenAI, Qwen2.5, LangChain
- **Function-Specific Models**: Different models for different tasks
- **Intelligent Fallbacks**: Automatic failover to cheaper models

### Token Tracking & Cost Management
- SQLite-backed tracking
- Real-time cost calculations
- Comprehensive reporting
- CSV export capability

### Resilience & Reliability
- Circuit breakers prevent cascading failures
- Exponential backoff with jitter
- Error classification and retry logic
- Comprehensive error handling

---

## ğŸ”§ Development Guide

### Adding a New LLM Provider

The Registry Pattern makes adding new LLM providers incredibly simple:

#### Example: Adding Claude (Anthropic)

1. **Create the adapter** (`app/adapters/providers/claude.py`):
```python
from app.adapters.base import BaseLLMAdapter
import requests

class ClaudeAdapter(BaseLLMAdapter):
    def generate(self, prompt: str, **kwargs) -> str:
        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/json"
        }
        payload = {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": kwargs.get("num_predict", 512)
        }
        
        response = requests.post(
            f"{self.base_url}/v1/messages",
            json=payload,
            headers=headers
        )
        response.raise_for_status()
        return response.json()["content"][0]["text"]
```

2. **Register the provider** (add to `app/adapters/llm_adapter.py`):
```python
from app.adapters.providers.claude import ClaudeAdapter
LLMProviderRegistry.register("claude", ClaudeAdapter)
```

3. **Use it**:
```bash
LLM_PROVIDER=claude
LLM_MODEL=claude-3-sonnet-20240229
```

**Total effort**: 1 new file + 2 lines of code! ğŸ‰

---

## ğŸ› ï¸ Technical Requirements

* **Python** â‰¥ 3.10
* **FastAPI** â‰¥ 0.115
* **Pydantic** â‰¥ 2.0
* **LangChain** â‰¥ 0.3.0 (for advanced features)
* **Pytest** for testing
* **(Optional)** Ollama for local development
  ```bash
  ollama pull llama3.1
  ollama serve
  ```
* **(Optional)** Qwen2.5 models via Ollama
  ```bash
  ollama pull qwen2.5
  ollama serve
  ```

---

## ğŸ” Environment Variables

### Required
- `LLM_PROVIDER`: LLM provider (ollama/openai/qwen2.5/langchain)
- `LLM_MODEL`: Default model to use

### Optional
- `OPENAI_API_KEY`: OpenAI API key (required for production)
- `DEFAULT_LANGUAGE`: Default response language (en/es)
- `ENABLE_TOKEN_TRACKING`: Enable usage tracking (true/false)
- `TOKEN_TRACKING_DB_PATH`: Path to tracking database

### Function-Specific Models
- `LLM_MODEL_AI_DETECTION`: Model for AI detection (e.g., "openai:gpt-4o")
- `LLM_MODEL_FEEDBACK`: Model for essay feedback (e.g., "openai:gpt-4o-mini")
- `LLM_MODEL_GUIDANCE`: Model for section guidance (e.g., "qwen2.5:qwen2.5:3b")
- `LLM_MODEL_SECTION_CHECK`: Model for section checking (e.g., "qwen2.5:qwen2.5:3b")

### Fallback Configuration
- `LLM_FALLBACK_*`: Comma-separated fallback chains for each function
- `*_MAX_RETRIES`: Maximum retries per model type
- `CIRCUIT_BREAKER_*`: Circuit breaker settings

See `.env.example` for complete configuration options.

---

## ğŸ—ºï¸ Development Roadmap

| Phase | Goal | Status |
|:------|:-----|:-------|
| A | Base structure, models, routers, healthcheck | âœ… |
| B | Implement prompts and utilities | âœ… |
| C | LLM adapters (Ollama, OpenAI) | âœ… |
| D | Services: analyzer and guidance | âœ… |
| E | Unit & integration tests | âœ… |
| F | Token tracking and fallback system | âœ… |
| G | Modular refactoring (prompts, tracking, fallbacks) | âœ… |
| H | Registry Pattern implementation | âœ… |
| I | **LangChain Integration** | âœ… |
| J | Docker + CI/CD + deployment | ğŸ”„ |

---

## ğŸš€ Future LangChain Opportunities

### ğŸ§  **Phase 1: Memory & Conversational Analysis**
- **Conversational Memory**: Track student progress across multiple essay submissions
- **Learning Path Analysis**: Understand student improvement patterns
- **Personalized Feedback**: Adapt recommendations based on student history
- **Session Continuity**: Maintain context across analysis sessions

### ğŸ” **Phase 2: RAG (Retrieval Augmented Generation)**
- **Knowledge Base Integration**: Connect to educational databases
- **Citation Analysis**: Verify and suggest academic sources
- **Plagiarism Detection**: Compare against known academic works
- **Topic-Specific Guidance**: Provide domain-specific writing advice

### ğŸ¤– **Phase 3: Agent-Based Workflows**
- **Writing Assistant Agent**: Multi-step essay improvement workflow
- **Research Agent**: Automated fact-checking and source validation
- **Style Analysis Agent**: Advanced writing style and tone analysis
- **Peer Review Agent**: Simulate peer review processes

### ğŸ“Š **Phase 4: Advanced Analytics**
- **Learning Analytics**: Track student writing development over time
- **Classroom Insights**: Aggregate analysis for teachers
- **Writing Pattern Recognition**: Identify common student challenges
- **Predictive Feedback**: Anticipate areas needing improvement

### ğŸ”— **Phase 5: Integration Ecosystem**
- **LMS Integration**: Connect with Learning Management Systems
- **Google Classroom**: Direct integration with Google Workspace
- **Canvas/Moodle**: Support for popular educational platforms
- **API Ecosystem**: Third-party integrations and plugins

---

## ğŸ“Š Features in Detail

### ğŸš€ LangChain Advanced Features
- **Multi-step Analysis**: Sequential processing with context accumulation
- **PydanticOutputParser**: Automatic structured output validation
- **Context Awareness**: Each analysis step builds upon previous results
- **Granular Evaluation**: Individual chains for each Toulmin criterion
- **Error Resilience**: Automatic JSON cleaning and type conversion

### Token Tracking System
- Automatic token counting for all LLM calls
- Cost calculation based on provider and model
- SQLite storage for persistent tracking
- Query by date range, function, or provider
- Export data for external analysis

### Fallback System
- Automatic model degradation on failure
- Configurable fallback chains per function
- Retry logic with exponential backoff
- Circuit breakers for failing models
- Error classification (retryable vs non-retryable)

### Cost Optimization
- Function-specific model selection
- Use cheaper models for simpler tasks
- Reserve premium models for critical functions
- Daily/monthly cost reporting
- Usage trends analysis

---

## ğŸ“ Educational Impact

### For Students
- **Detailed Feedback**: Comprehensive analysis using Extended Toulmin Model
- **Learning Progression**: Track improvement over time
- **Personalized Guidance**: Adapt to individual writing styles
- **Academic Standards**: Align with university-level evaluation criteria

### For Educators
- **Consistent Evaluation**: Standardized criteria across all essays
- **Time Efficiency**: Automated analysis frees time for teaching
- **Classroom Insights**: Aggregate data for curriculum improvement
- **Scalable Assessment**: Handle large class sizes efficiently

### For Institutions
- **Quality Assurance**: Maintain academic standards
- **Resource Optimization**: Reduce manual grading workload
- **Data-Driven Decisions**: Analytics for curriculum development
- **Technology Integration**: Modern AI-powered educational tools

---

## ğŸ‘¨â€ğŸ’» Author

**Fernando Herrera SÃ¡nchez**  
Software Engineer ğŸ’¡  
ğŸ“ MÃ©xico

**GitHub**: [@fernando-herrera](https://github.com/fernandohs)  
**LinkedIn**: [Fernando Herrera](https://www.linkedin.com/in/fernando-herrera-sanchez-90699859/)

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

### ğŸš€ LangChain Contributions
We especially welcome contributions related to:
- New LangChain features and integrations
- Advanced analysis workflows
- Memory and RAG implementations
- Agent-based systems
- Educational AI applications

---

## â­ Star History

If you find this project useful, please consider giving it a star! â­

---

## ğŸ”— Related Projects

- [LangChain Documentation](https://python.langchain.com/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Extended Toulmin Model](https://en.wikipedia.org/wiki/Toulmin_model)
- [Ollama Models](https://ollama.ai/)

---

## ğŸ“ˆ Project Metrics

- **Lines of Code**: 2,500+
- **Test Coverage**: 90%+
- **API Endpoints**: 15+
- **LLM Providers**: 4+ (Ollama, OpenAI, Qwen2.5, LangChain)
- **Supported Languages**: 2 (English, Spanish)
- **Evaluation Criteria**: 7 (Extended Toulmin Model)
- **Test Cases**: 107+

---

## ğŸ† Achievements

- âœ… **Production Ready**: Enterprise-grade features and reliability
- âœ… **LangChain Integration**: Advanced AI workflows and chains
- âœ… **Comprehensive Testing**: 90%+ test coverage
- âœ… **Modular Architecture**: Clean, maintainable, extensible code
- âœ… **Multi-Provider Support**: Flexible LLM provider system
- âœ… **Educational Focus**: Purpose-built for academic essay analysis
- âœ… **Open Source**: MIT licensed for educational use