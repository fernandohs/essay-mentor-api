# 🧠 Essay Mentor API

> **FastAPI + Modular Architecture + LLM Adapters (Ollama / OpenAI) with Token Tracking & Fallback System**

A production-ready FastAPI service designed to **analyze and guide** the writing of short student essays (max. 8,000 characters). Built with clean architecture principles, comprehensive testing, and enterprise-grade features.

The service provides automated tools to:

* **Estimate** whether a text may have been generated by AI.
* Offer **personalized feedback** based on multiple criteria (originality, creativity, coherence, etc.).
* **Guide** students with section-specific explanations and writing tips for argumentative essays based on the **Toulmin model**:
    * *Claim*
    * *Reasoning*
    * *Evidence*
    * *Backing*
    * *Reservation*
    * *Rebuttal*

---

## ✨ Key Features

### Core Features
* **Modular architecture** with clear separation of concerns (routers, services, adapters, prompts, models, utils)
* **Interchangeable LLM adapters** (Ollama for local development, OpenAI for production)
* **Swagger-ready endpoints** with comprehensive API documentation
* **Configurable prompts** with educational rules (never generate the student's answer)
* **Structured feedback** based on customizable evaluation criteria
* **Bilingual-ready**: Configure default language via `DEFAULT_LANGUAGE` env variable (en/es)

### Enterprise Features
* **Token usage tracking** with SQLite database
* **Cost tracking** for OpenAI usage with daily/monthly reports
* **Intelligent fallback system** for OpenAI models (gpt-4o → gpt-4o-mini → gpt-3.5-turbo)
* **Retry logic** with exponential backoff and jitter
* **Circuit breakers** to prevent cascading failures
* **Function-specific model selection** for cost optimization
* **Usage analytics** API with CSV export capability

---

## 🏗️ Project Structure

```
EssayMentorApi/
├── app/
│   ├── main.py                           # FastAPI application entry point
│   ├── core/
│   │   └── config.py                     # Configuration management
│   ├── adapters/
│   │   ├── base.py                       # Base LLM adapter interface
│   │   ├── registry.py                   # Provider registry system
│   │   ├── llm_adapter.py                # Unified LLM adapter wrapper
│   │   └── providers/                    # Provider-specific adapters
│   │       ├── ollama.py                 # Ollama adapter (local models)
│   │       └── openai.py                 # OpenAI adapter (API models)
│   ├── models/
│   │   ├── types.py                      # Section enums and mappings
│   │   ├── essay.py                      # Essay request/response models
│   │   ├── analyze.py                    # AI detection & feedback models
│   │   ├── guide.py                      # Guidance models
│   │   └── usage.py                      # Token usage tracking models
│   ├── prompts/
│   │   ├── factory.py                    # Prompt generation wrapper
│   │   ├── generators/                   # Modular prompt components
│   │   │   ├── constants.py             # Educational rules & constants
│   │   │   ├── language_handler.py      # Multi-language support
│   │   │   ├── section_definitions.py   # Toulmin model sections
│   │   │   ├── templates.py             # Prompt templates
│   │   │   └── __init__.py              # Main generator class
│   │   └── criteria_data.py             # Evaluation criteria
│   ├── services/
│   │   ├── analyzer.py                   # AI detection & feedback logic
│   │   └── guidance.py                   # Essay section guidance logic
│   ├── routers/
│   │   ├── meta.py                       # Health & metadata endpoints
│   │   ├── analyze.py                    # Analysis endpoints
│   │   ├── guide.py                      # Guidance endpoints
│   │   └── usage.py                      # Token usage & reporting endpoints
│   └── utils/
│       ├── json_parse.py                 # Robust JSON parsing
│       ├── criteria.py                   # Criteria validation
│       ├── text_format.py                # Text formatting utilities
│       ├── token_tracker.py              # Token tracking wrapper
│       ├── fallback_manager.py           # Fallback system wrapper
│       ├── tracking/                     # Token tracking components
│       │   ├── database_manager.py      # SQLite operations
│       │   ├── cost_calculator.py       # Cost calculations
│       │   ├── report_generator.py      # Usage reports
│       │   └── __init__.py              # Main tracker class
│       └── fallback/                     # Fallback system components
│           ├── circuit_breaker.py       # Circuit breaker logic
│           ├── retry_logic.py           # Retry strategies
│           ├── error_classifier.py      # Error classification
│           └── __init__.py              # Main fallback manager
├── tests/                                # Comprehensive test suite
│   ├── conftest.py                      # Shared fixtures
│   ├── test_adapters_llm.py            # Adapter tests
│   ├── test_services_analyzer.py       # Analyzer tests
│   ├── test_services_guidance.py       # Guidance tests
│   ├── test_utils_json_parse.py        # Utils tests
│   ├── test_utils_criteria.py          # Criteria tests
│   ├── test_utils_text_format.py       # Text format tests
│   └── test_routers_integration.py     # Integration tests
├── examples/                             # Usage examples
├── requirements.txt                      # Python dependencies
├── .env.example                          # Environment variables template
└── README.md                             # This file
```

---

## 🚀 Quick Start

### 1. Clone the repository

```bash
git clone https://github.com/<your-username>/EssayMentorApi.git
cd EssayMentorApi
```

### 2. Create virtual environment

```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Configure environment variables

Create a `.env` file based on `.env.example`:

```bash
# Application Configuration
APP_NAME=Essay Mentor API
APP_VERSION=1.0.0
HOST=0.0.0.0
PORT=8000
CORS_ORIGINS=*

# LLM Configuration
LLM_PROVIDER=ollama  # or 'openai' for production, 'qwen2.5' for Qwen models
LLM_MODEL=llama3.1   # Default model
OLLAMA_URL=http://localhost:11434

# OpenAI Configuration (for production)
OPENAI_API_KEY=your_key_here
OPENAI_BASE_URL=https://api.openai.com/v1

# Function-specific model selection (cost optimization)
LLM_MODEL_AI_DETECTION=   # Empty = use LLM_MODEL
LLM_MODEL_FEEDBACK=        # Empty = use LLM_MODEL
LLM_MODEL_GUIDANCE=        # Empty = use LLM_MODEL
LLM_MODEL_SECTION_CHECK=   # Empty = use LLM_MODEL

# Fallback Configuration (OpenAI models only)
LLM_FALLBACK_AI_DETECTION=gpt-4o,gpt-4o-mini,gpt-3.5-turbo
LLM_FALLBACK_FEEDBACK=gpt-4o,gpt-4o-mini,gpt-3.5-turbo
LLM_FALLBACK_GUIDANCE=gpt-4o-mini,gpt-3.5-turbo
LLM_FALLBACK_SECTION_CHECK=gpt-4o-mini,gpt-3.5-turbo

# Retry Configuration
GPT4O_MAX_RETRIES=2
GPT4O_MINI_MAX_RETRIES=3
GPT35_TURBO_MAX_RETRIES=5

# Circuit Breaker Configuration
CIRCUIT_BREAKER_FAILURE_THRESHOLD=5
CIRCUIT_BREAKER_RECOVERY_TIMEOUT=300

# Language Configuration
DEFAULT_LANGUAGE=en  # en=English, es=Spanish

# Token Tracking Configuration
ENABLE_TOKEN_TRACKING=true
TOKEN_TRACKING_DB_PATH=usage_tracking.db
```

### 5. Run the API

```bash
# Development
uvicorn app.main:app --reload

# Production
gunicorn app.main:app -w 4 -k uvicorn.workers.UvicornWorker
```

**Access Points:**
* **Swagger UI:** `http://localhost:8000/docs`
* **ReDoc:** `http://localhost:8000/redoc`
* **OpenAPI JSON:** `http://localhost:8000/openapi.json`

---

## 📚 API Endpoints

### Analysis Endpoints

| Method | Route | Description |
|:---|:---|:---|
| `POST` | `/analyze/ai-likelihood` | Estimate AI-generated likelihood (0-100 score) |
| `POST` | `/analyze/feedback` | Generate structured writing feedback with criteria evaluation |

### Guidance Endpoints

| Method | Route | Description |
|:---|:---|:---|
| `POST` | `/guide` | Provide guidance for essay sections (claim, reasoning, etc.) |
| `POST` | `/guide/check-section` | Review a specific section and suggest improvements |

### Usage & Reporting Endpoints

| Method | Route | Description |
|:---|:---|:---|
| `GET` | `/admin/usage/daily` | Get daily token usage report |
| `GET` | `/admin/usage/report` | Get usage report for date range |
| `GET` | `/admin/usage/costs` | Get cost summary |
| `GET` | `/admin/usage/trends` | Get usage trends |
| `GET` | `/admin/usage/export/csv` | Export usage data as CSV |

### Meta Endpoints

| Method | Route | Description |
|:---|:---|:---|
| `GET` | `/health` | Check API health and LLM provider status |

---

## 📖 Example Usage

### Analyze AI Likelihood

```bash
curl -X POST "http://localhost:8000/analyze/ai-likelihood" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "This essay argues that technology enhances education...",
    "language": "en"
  }'
```

**Response:**
```json
{
  "score": 35,
  "rationale": "The text shows natural variations in style...",
  "caveats": ["Analysis is probabilistic", "Context matters"]
}
```

### Get Essay Feedback

```bash
curl -X POST "http://localhost:8000/analyze/feedback" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Students should learn critical thinking...",
    "criteria": ["originalidad", "coherencia"],
    "language": "es"
  }'
```

### Get Section Guidance

```bash
curl -X POST "http://localhost:8000/guide" \
  -H "Content-Type: application/json" \
  -d '{
    "section": "claim",
    "language": "en"
  }'
```

### Check Usage Statistics

```bash
# Daily usage
curl "http://localhost:8000/admin/usage/daily"

# Cost summary (last 7 days)
curl "http://localhost:8000/admin/usage/costs?days=7"

# Export to CSV
curl "http://localhost:8000/admin/usage/export/csv?start_date=2024-01-01&end_date=2024-01-31"
```

---

## ⚙️ LLM Provider Configuration

### Registry Pattern Architecture

The project uses a **Registry Pattern** for LLM providers, making it extremely easy to add new providers without modifying existing code.

#### Supported Providers

| Provider | Description | Setup | Adapter |
|:---------|:------------|:------|:--------|
| `ollama` | Local Ollama models (Llama, Mistral, etc.) | `ollama pull llama3.1 && ollama serve` | OllamaAdapter |
| `openai` | OpenAI API models (GPT-4, GPT-3.5) | Set `OPENAI_API_KEY` environment variable | OpenAIAdapter |
| `qwen2.5` | Qwen2.5 models via Ollama | `ollama pull qwen2.5 && ollama serve` | OllamaAdapter |

### 🚀 Adding New LLM Providers

Adding a new LLM provider is now **incredibly simple** - just 2 steps:

#### Step 1: Create Provider Adapter
Create `app/adapters/providers/your_provider.py`:

```python
from app.adapters.base import BaseLLMAdapter

class YourProviderAdapter(BaseLLMAdapter):
    def generate(self, prompt: str, **kwargs) -> str:
        # Your provider-specific implementation
        # Handle API calls, authentication, response parsing
        return response_text
```

#### Step 2: Register the Provider
Add 2 lines to `app/adapters/llm_adapter.py`:

```python
from app.adapters.providers.your_provider import YourProviderAdapter
LLMProviderRegistry.register("your_provider", YourProviderAdapter)
```

#### Step 3: Use It! 🎉
```bash
LLM_PROVIDER=your_provider
LLM_MODEL=your_model_name
```

**That's it!** No other files need to be modified.

### Configuration Examples

#### Development with Ollama
```bash
LLM_PROVIDER=ollama
LLM_MODEL=llama3.1
```

#### Development with Qwen2.5
```bash
LLM_PROVIDER=qwen2.5
LLM_MODEL=qwen2.5:3b
```

#### Production with OpenAI
```bash
LLM_PROVIDER=openai
LLM_MODEL=gpt-4o-mini
OPENAI_API_KEY=your_api_key_here
```

#### Mixed Configuration (Cost Optimization)
```bash
LLM_PROVIDER=openai
LLM_MODEL=gpt-4o-mini
LLM_MODEL_AI_DETECTION=openai:gpt-4o          # Premium model for critical tasks
LLM_MODEL_FEEDBACK=openai:gpt-4o-mini         # Balanced model for feedback
LLM_MODEL_GUIDANCE=qwen2.5:qwen2.5:3b         # Free model for guidance
LLM_MODEL_SECTION_CHECK=qwen2.5:qwen2.5:3b    # Free model for section checks
```

### 🔄 Fallback System

The system includes a sophisticated fallback mechanism for OpenAI models:

#### How It Works
1. **Primary Model**: Attempts to use the configured model
2. **Fallback Chain**: If primary fails, tries cheaper models in sequence
3. **Circuit Breaker**: Temporarily stops requests to failing models
4. **Retry Logic**: Exponential backoff with jitter for transient failures

#### Fallback Configuration
```bash
# Define fallback chains for each function
LLM_FALLBACK_AI_DETECTION=gpt-4o,gpt-4o-mini,gpt-3.5-turbo
LLM_FALLBACK_FEEDBACK=gpt-4o,gpt-4o-mini,gpt-3.5-turbo
LLM_FALLBACK_GUIDANCE=gpt-4o-mini,gpt-3.5-turbo
LLM_FALLBACK_SECTION_CHECK=gpt-4o-mini,gpt-3.5-turbo

# Retry settings
GPT4O_MAX_RETRIES=2
GPT4O_MINI_MAX_RETRIES=3
GPT35_TURBO_MAX_RETRIES=5

# Circuit breaker settings
CIRCUIT_BREAKER_FAILURE_THRESHOLD=5
CIRCUIT_BREAKER_RECOVERY_TIMEOUT=300  # 5 minutes
```

#### Fallback Benefits
- ✅ **Cost Optimization**: Automatically uses cheaper models when possible
- ✅ **Reliability**: Continues working even if premium models fail
- ✅ **Performance**: Circuit breakers prevent cascading failures
- ✅ **Monitoring**: Full tracking of fallback usage and success rates

---

## 🧪 Testing

The project includes comprehensive test coverage with 107 tests.

### Run Tests

```bash
# All tests
pytest

# With verbose output
pytest -v

# With coverage report
pytest --cov=app --cov-report=html

# Specific test file
pytest tests/test_services_analyzer.py

# Specific test
pytest tests/test_services_analyzer.py::test_analyze_ai_likelihood_success
```

### Test Structure

- **Unit Tests**: Services, adapters, utilities
- **Integration Tests**: API endpoints
- **Fixtures**: Mock LLM responses
- **Coverage**: ~90% code coverage

**Current Status:** ✅ 107 tests passing

---

## 🎯 Architecture Highlights

### Clean Architecture
- **Routers** → **Services** → **Adapters** → **Models**
- Clear separation of concerns
- No circular dependencies
- Testable components

### Modular Prompt Generation
- Separated concerns (constants, languages, sections, templates)
- Reusable components
- Easy to extend and maintain

### Robust LLM Integration
- **Registry Pattern**: Easy to add new LLM providers
- **Unified Interface**: Single API for all providers
- **Multi-Provider Support**: Ollama, OpenAI, Qwen2.5, and more
- **Function-Specific Models**: Different models for different tasks
- **Intelligent Fallbacks**: Automatic failover to cheaper models

### Token Tracking & Cost Management
- SQLite-backed tracking
- Real-time cost calculations
- Comprehensive reporting
- CSV export capability

### Resilience & Reliability
- Circuit breakers prevent cascading failures
- Exponential backoff with jitter
- Error classification and retry logic
- Comprehensive error handling

### Registry Pattern Benefits
- **Extensibility**: Add new LLM providers without modifying existing code
- **Maintainability**: Each provider is isolated and independently testable
- **Scalability**: Easy to support multiple providers and aliases
- **Backward Compatibility**: Existing code continues to work unchanged

---

## 🔧 Development Guide

### Adding a New LLM Provider

The Registry Pattern makes adding new LLM providers incredibly simple:

#### Example: Adding Claude (Anthropic)

1. **Create the adapter** (`app/adapters/providers/claude.py`):
```python
from app.adapters.base import BaseLLMAdapter
import requests

class ClaudeAdapter(BaseLLMAdapter):
    def generate(self, prompt: str, **kwargs) -> str:
        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/json"
        }
        payload = {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": kwargs.get("num_predict", 512)
        }
        
        response = requests.post(
            f"{self.base_url}/v1/messages",
            json=payload,
            headers=headers
        )
        response.raise_for_status()
        return response.json()["content"][0]["text"]
```

2. **Register the provider** (add to `app/adapters/llm_adapter.py`):
```python
from app.adapters.providers.claude import ClaudeAdapter
LLMProviderRegistry.register("claude", ClaudeAdapter)
```

3. **Use it**:
```bash
LLM_PROVIDER=claude
LLM_MODEL=claude-3-sonnet-20240229
```

**Total effort**: 1 new file + 2 lines of code! 🎉

### Provider Aliases

You can create aliases for providers:

```python
# Register aliases
LLMProviderRegistry.register_alias("gpt4", "openai")
LLMProviderRegistry.register_alias("llama", "ollama")

# Use aliases
adapter = get_llm_adapter(provider="gpt4", model="gpt-4o")
```

---

## 🛠️ Technical Requirements

* **Python** ≥ 3.10
* **FastAPI** ≥ 0.115
* **Pydantic** ≥ 2.0
* **Pytest** for testing
* **(Optional)** Ollama for local development
  ```bash
  ollama pull llama3.1
  ollama serve
  ```
* **(Optional)** Qwen2.5 models via Ollama
  ```bash
  ollama pull qwen2.5
  ollama serve
  ```

---

## 🔐 Environment Variables

### Required
- `LLM_PROVIDER`: LLM provider (ollama/openai/qwen2.5)
- `LLM_MODEL`: Default model to use

### Optional
- `OPENAI_API_KEY`: OpenAI API key (required for production)
- `DEFAULT_LANGUAGE`: Default response language (en/es)
- `ENABLE_TOKEN_TRACKING`: Enable usage tracking (true/false)
- `TOKEN_TRACKING_DB_PATH`: Path to tracking database

### Function-Specific Models
- `LLM_MODEL_AI_DETECTION`: Model for AI detection (e.g., "openai:gpt-4o")
- `LLM_MODEL_FEEDBACK`: Model for essay feedback (e.g., "openai:gpt-4o-mini")
- `LLM_MODEL_GUIDANCE`: Model for section guidance (e.g., "qwen2.5:qwen2.5:3b")
- `LLM_MODEL_SECTION_CHECK`: Model for section checking (e.g., "qwen2.5:qwen2.5:3b")

### Fallback Configuration
- `LLM_FALLBACK_*`: Comma-separated fallback chains for each function
- `*_MAX_RETRIES`: Maximum retries per model type
- `CIRCUIT_BREAKER_*`: Circuit breaker settings

See `.env.example` for complete configuration options.

---

## 🗺️ Development Roadmap

| Phase | Goal | Status |
|:------|:-----|:-------|
| A | Base structure, models, routers, healthcheck | ✅ |
| B | Implement prompts and utilities | ✅ |
| C | LLM adapters (Ollama, OpenAI) | ✅ |
| D | Services: analyzer and guidance | ✅ |
| E | Unit & integration tests | ✅ |
| F | Token tracking and fallback system | ✅ |
| G | Modular refactoring (prompts, tracking, fallbacks) | ✅ |
| H | Registry Pattern implementation | ✅ |
| I | Docker + CI/CD + deployment | 🔄 |

---

## 📊 Features in Detail

### Token Tracking System
- Automatic token counting for all LLM calls
- Cost calculation based on provider and model
- SQLite storage for persistent tracking
- Query by date range, function, or provider
- Export data for external analysis

### Fallback System
- Automatic model degradation on failure
- Configurable fallback chains per function
- Retry logic with exponential backoff
- Circuit breakers for failing models
- Error classification (retryable vs non-retryable)

### Cost Optimization
- Function-specific model selection
- Use cheaper models for simpler tasks
- Reserve premium models for critical functions
- Daily/monthly cost reporting
- Usage trends analysis

---

## 👨‍💻 Author

**Fernando Herrera Sánchez**  
Software Engineer 💡  
📍 México

---

## 📄 License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## 🤝 Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ⭐ Star History

If you find this project useful, please consider giving it a star! ⭐
