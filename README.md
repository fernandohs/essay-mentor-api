# ğŸ§  Essay Mentor API

> **FastAPI + Modular Architecture + LLM Adapters (Ollama / GPT-ready)**

A modular FastAPI service designed to **analyze and guide** the writing of short student essays (max. 8,000 characters).

Its goal is to provide automated tools to:

  * **Estimate** whether a text may have been generated by AI.
  * Offer **personalized feedback** based on multiple criteria (originality, creativity, coherence, etc.).
  * **Guide** students with section-specific explanations and writing tips for argumentative essays based on the **Toulmin model** (or similar):
      * *Claim*
      * *Reasoning*
      * *Evidence*
      * *Backing*
      * *Reservation*
      * *Rebuttal*

The project currently uses local models (like Llama 3 through **Ollama**) and can easily connect to GPT models in the future via its adapter layer.

-----

## ğŸš€ Key Features

  * **Modular architecture** (routers, services, adapters, prompts, models, utils).
  * **Interchangeable LLM adapters** (Ollama, OpenAI, etc.).
  * **Swagger-ready endpoints** (`/analyze`, `/guide`, `/health`).
  * **Configurable prompts** with educational rules (never generate the studentâ€™s answer).
  * **Structured feedback** based on customizable evaluation criteria.
  * **Bilingual-ready** (supports both Spanish and English for section names).

-----

## ğŸ§© Project Structure

```
essay-mentor-api/
â”œâ”€ app/
â”‚  â”œâ”€ main.py
â”‚  â”œâ”€ core/config.py
â”‚  â”œâ”€ adapters/
â”‚  â”‚  â”œâ”€ llm_base.py
â”‚  â”‚  â”œâ”€ ollama_adapter.py
â”‚  â”‚  â””â”€ openai_adapter.py
â”‚  â”œâ”€ models/
â”‚  â”‚  â”œâ”€ types.py            # Section (en/es) and mappings
â”‚  â”‚  â”œâ”€ essay.py
â”‚  â”‚  â”œâ”€ analyze.py
â”‚  â”‚  â””â”€ guide.py
â”‚  â”œâ”€ prompts/factory.py
â”‚  â”œâ”€ services/
â”‚  â”‚  â”œâ”€ analyzer.py
â”‚  â”‚  â””â”€ guidance.py
â”‚  â”œâ”€ routers/
â”‚  â”‚  â”œâ”€ analyze.py
â”‚  â”‚  â”œâ”€ guide.py
â”‚  â”‚  â””â”€ meta.py
â”‚  â””â”€ utils/json_parse.py
â”œâ”€ tests/
â”œâ”€ requirements.txt
â”œâ”€ .env.example
â”œâ”€ .gitignore
â””â”€ README.md
```

-----

## âš™ï¸ Installation

### 1\. Clone the repository

```bash
git clone https://github.com/<your-username>/essay-mentor-api.git
cd essay-mentor-api
```

### 2\. Create virtual environment

```bash
python -m venv .venv
source .venv/bin/activate  # (Windows: .venv\Scripts\activate)
```

### 3\. Install dependencies

```bash
pip install -r requirements.txt
```

### 4\. Configure environment variables

```bash
cp .env.example .env
# Edit values for Ollama or GPT usage
```

-----

## â–¶ï¸ Run the API

```bash
uvicorn app.main:app --reload
```

  * **Swagger UI:**
    ğŸ‘‰ `http://localhost:8000/docs`
  * **OpenAPI JSON:**
    ğŸ‘‰ `http://localhost:8000/openapi.json`

-----

## ğŸ§  Main Endpoints

| Method | Route | Description |
|:---|:---|:---|
| `GET` | `/health` | Check API health and LLM provider |
| `POST` | `/analyze/ai-likelihood` | Estimate AI-generated likelihood |
| `POST` | `/analyze/feedback` | Generate structured writing feedback |
| `POST` | `/guide` | Provide guidance for essay sections |
| `POST` | `/guide/check-section` | Review a section and suggest improvements |

-----

## ğŸ§© Example Request

```json
POST /analyze/ai-likelihood
Content-Type: application/json

{
  "text": "This essay argues that empathy plays a key role in education.",
  "section": "claim"
}
```

-----

## ğŸ§° Technical Requirements

  * Python $\ge$ 3.10
  * FastAPI $\ge$ 0.115
  * **(Optional)** Ollama running locally at `http://localhost:11434`
    ```bash
    ollama pull llama3.1
    ollama serve
    ```

-----

## ğŸ§± Best Practices

  * Clean Architecture: `routers` â†’ `services` â†’ `adapters` â†’ `models`.
  * Environment-based configuration using `pydantic-settings`.
  * Strong Pydantic validation: character limits, literals, nested models.
  * Robust JSON parsing from LLM responses.
  * Bilingual support (`en`/`es`) for essay sections.
  * No circular dependencies.
  * Ready for Docker or production deployment with `uvicorn`/`gunicorn`.

-----

## ğŸ§ª Unit Testing (Optional)

```bash
pytest -q
```

Recommended to use Httpx for endpoint testing:

```python
from httpx import AsyncClient
from app.main import app

async def test_health():
    async with AsyncClient(app=app, base_url="http://test") as ac:
        res = await ac.get("/health")
        assert res.status_code == 200
```

-----

## ğŸ§­ Development Roadmap

| Phase | Goal |
|:------|:-----|
| A | Base structure, models, routers, healthcheck âœ… |
| B | Implement prompts and utils/json\_parse.py |
| C | LLM adapters (Ollama, OpenAI) |
| D | Services: analyzer.py and guidance.py |
| E | Unit & integration tests |
| F | Dockerfile + CI/CD + deployment |

-----

## ğŸ‘¨â€ğŸ’» Author

Fernando Herrera SÃ¡nchez
Software Engineer ğŸ’¡
ğŸ“ MÃ©xico
