# 🧠 Essay Mentor API

> **FastAPI + LangChain + Modular Architecture + LLM Adapters with Advanced AI Features**

A production-ready FastAPI service designed to **analyze and guide** the writing of short student essays (max. 8,000 characters). Built with clean architecture principles, comprehensive testing, enterprise-grade features, and **advanced LangChain integration** for sophisticated AI workflows.

The service provides automated tools to:

* **Estimate** whether a text may have been generated by AI.
* Offer **personalized feedback** based on multiple criteria (originality, creativity, coherence, etc.).
* **Advanced analysis** using LangChain chains for detailed, multi-step evaluation.
* **Guide** students with section-specific explanations and writing tips for argumentative essays based on the **Extended Toulmin Model**:
    * *Originality* (22 points)
    * *Depth* (18 points) 
    * *Integrality* (16 points)
    * *Conciliation* (14 points)
    * *Refutation* (12 points)
    * *Evidence* (10 points)
    * *Logic* (8 points)

---

## ✨ Key Features

### Core Features
* **Modular architecture** with clear separation of concerns (routers, services, adapters, prompts, models, utils)
* **Interchangeable LLM adapters** (Ollama for local development, OpenAI for production, LangChain for advanced features)
* **Swagger-ready endpoints** with comprehensive API documentation
* **Configurable prompts** with educational rules (never generate the student's answer)
* **Structured feedback** based on customizable evaluation criteria
* **Bilingual-ready**: Configure default language via `DEFAULT_LANGUAGE` env variable (en/es)

### 🚀 Advanced LangChain Features
* **Multi-step analysis chains** for comprehensive essay evaluation
* **PydanticOutputParser** for robust, structured LLM responses
* **Context-aware processing** with accumulated knowledge across analysis steps
* **Granular criterion evaluation** with individual LangChain chains
* **Advanced error handling** with automatic JSON cleaning and validation
* **Memory integration** (ready for conversational analysis)
* **RAG capabilities** (ready for knowledge base integration)
* **Agent workflows** (ready for complex reasoning tasks)

### Enterprise Features
* **Token usage tracking** with SQLite database
* **Cost tracking** for OpenAI usage with daily/monthly reports
* **Intelligent fallback system** for OpenAI models (gpt-4o → gpt-4o-mini → gpt-3.5-turbo)
* **Retry logic** with exponential backoff and jitter
* **Circuit breakers** to prevent cascading failures
* **Function-specific model selection** for cost optimization
* **Usage analytics** API with CSV export capability

---

## 🏗️ Project Structure

```
EssayMentorApi/
├── app/
│   ├── main.py                           # FastAPI application entry point
│   ├── core/
│   │   └── config.py                     # Configuration management
│   ├── adapters/
│   │   ├── base.py                       # Base LLM adapter interface
│   │   ├── registry.py                   # Provider registry system
│   │   ├── llm_adapter.py                # Unified LLM adapter wrapper
│   │   └── providers/                    # Provider-specific adapters
│   │       ├── ollama.py                 # Ollama adapter (local models)
│   │       ├── openai.py                 # OpenAI adapter (API models)
│   │       └── langchain_adapter.py      # LangChain adapter (advanced features)
│   ├── models/
│   │   ├── types.py                      # Section enums and mappings
│   │   ├── essay.py                      # Essay request/response models
│   │   ├── analyze.py                    # AI detection & feedback models
│   │   ├── guide.py                      # Guidance models
│   │   ├── criterion.py                  # Criterion evaluation models
│   │   └── usage.py                      # Token usage tracking models
│   ├── prompts/
│   │   ├── factory.py                    # Prompt generation wrapper
│   │   ├── criteria_data.py              # Extended Toulmin Model criteria
│   │   └── rubric_criteria.py            # Rubric definitions
│   ├── services/
│   │   ├── analyzer.py                   # AI detection & feedback logic
│   │   ├── guidance.py                   # Essay section guidance logic
│   │   └── advanced_analyzer.py          # LangChain-powered advanced analysis
│   ├── routers/
│   │   ├── meta.py                       # Health & metadata endpoints
│   │   ├── analyze.py                    # Analysis endpoints
│   │   ├── guide.py                      # Guidance endpoints
│   │   ├── advanced.py                   # Advanced LangChain endpoints
│   │   └── usage.py                      # Token usage & reporting endpoints
│   └── utils/
│       ├── json_parse.py                 # Robust JSON parsing
│       ├── criteria.py                   # Criteria validation
│       ├── text_format.py                # Text formatting utilities
│       ├── token_tracker.py              # Token tracking wrapper
│       ├── fallback_manager.py           # Fallback system wrapper
│       ├── tracking/                     # Token tracking components
│       │   ├── database_manager.py      # SQLite operations
│       │   ├── cost_calculator.py       # Cost calculations
│       │   ├── report_generator.py      # Usage reports
│       │   └── __init__.py              # Main tracker class
│       └── fallback/                     # Fallback system components
│           ├── circuit_breaker.py       # Circuit breaker logic
│           ├── retry_logic.py           # Retry strategies
│           ├── error_classifier.py      # Error classification
│           └── __init__.py              # Main fallback manager
├── tests/                                # Comprehensive test suite
│   ├── conftest.py                      # Shared fixtures
│   ├── test_adapters_llm.py            # Adapter tests
│   ├── test_services_analyzer.py       # Analyzer tests
│   ├── test_services_guidance.py       # Guidance tests
│   ├── test_services_advanced_analyzer.py # Advanced analyzer tests
│   ├── test_utils_json_parse.py        # Utils tests
│   ├── test_utils_criteria.py          # Criteria tests
│   ├── test_utils_text_format.py       # Text format tests
│   └── test_routers_integration.py     # Integration tests
├── examples/                             # Usage examples
├── requirements.txt                      # Python dependencies
├── .env.example                          # Environment variables template
└── README.md                             # This file
```

---

## 🚀 Quick Start

### 1. Clone the repository

```bash
git clone https://github.com/<your-username>/EssayMentorApi.git
cd EssayMentorApi
```

### 2. Create virtual environment

```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Configure environment variables

Create a `.env` file based on `.env.example`:

```bash
# Application Configuration
APP_NAME=Essay Mentor API
APP_VERSION=1.0.0
HOST=0.0.0.0
PORT=8000
CORS_ORIGINS=*

# LLM Configuration
LLM_PROVIDER=langchain  # or 'ollama', 'openai', 'qwen2.5'
LLM_MODEL=llama3.1      # Default model
OLLAMA_URL=http://localhost:11434

# OpenAI Configuration (for production)
OPENAI_API_KEY=your_key_here
OPENAI_BASE_URL=https://api.openai.com/v1

# Function-specific model selection (cost optimization)
LLM_MODEL_AI_DETECTION=   # Empty = use LLM_MODEL
LLM_MODEL_FEEDBACK=        # Empty = use LLM_MODEL
LLM_MODEL_GUIDANCE=        # Empty = use LLM_MODEL
LLM_MODEL_SECTION_CHECK=   # Empty = use LLM_MODEL

# Fallback Configuration (OpenAI models only)
LLM_FALLBACK_AI_DETECTION=gpt-4o,gpt-4o-mini,gpt-3.5-turbo
LLM_FALLBACK_FEEDBACK=gpt-4o,gpt-4o-mini,gpt-3.5-turbo
LLM_FALLBACK_GUIDANCE=gpt-4o-mini,gpt-3.5-turbo
LLM_FALLBACK_SECTION_CHECK=gpt-4o-mini,gpt-3.5-turbo

# Retry Configuration
GPT4O_MAX_RETRIES=2
GPT4O_MINI_MAX_RETRIES=3
GPT35_TURBO_MAX_RETRIES=5

# Circuit Breaker Configuration
CIRCUIT_BREAKER_FAILURE_THRESHOLD=5
CIRCUIT_BREAKER_RECOVERY_TIMEOUT=300

# Language Configuration
DEFAULT_LANGUAGE=en  # en=English, es=Spanish

# Token Tracking Configuration
ENABLE_TOKEN_TRACKING=true
TOKEN_TRACKING_DB_PATH=usage_tracking.db
```

### 5. Run the API

```bash
# Development
uvicorn app.main:app --reload

# Production
gunicorn app.main:app -w 4 -k uvicorn.workers.UvicornWorker
```

**Access Points:**
* **Swagger UI:** `http://localhost:8000/docs`
* **ReDoc:** `http://localhost:8000/redoc`
* **OpenAPI JSON:** `http://localhost:8000/openapi.json`

---

## 📚 API Endpoints

### Analysis Endpoints

| Method | Route | Description |
|:---|:---|:---|
| `POST` | `/analyze/ai-likelihood` | Estimate AI-generated likelihood (0-100 score) |
| `POST` | `/analyze/feedback` | Generate structured writing feedback with criteria evaluation |

### 🚀 Advanced LangChain Endpoints

| Method | Route | Description |
|:---|:---|:---|
| `POST` | `/advanced/analyze-chain` | **Advanced multi-step analysis** using LangChain chains with Extended Toulmin Model |
| `POST` | `/advanced/feedback-chains` | **Granular criterion analysis** with individual LangChain chains per criterion |

### Guidance Endpoints

| Method | Route | Description |
|:---|:---|:---|
| `POST` | `/guide` | Provide guidance for essay sections (claim, reasoning, etc.) |
| `POST` | `/guide/check-section` | Review a specific section and suggest improvements |

### Usage & Reporting Endpoints

| Method | Route | Description |
|:---|:---|:---|
| `GET` | `/admin/usage/daily` | Get daily token usage report |
| `GET` | `/admin/usage/report` | Get usage report for date range |
| `GET` | `/admin/usage/costs` | Get cost summary |
| `GET` | `/admin/usage/trends` | Get usage trends |
| `GET` | `/admin/usage/export/csv` | Export usage data as CSV |

### Meta Endpoints

| Method | Route | Description |
|:---|:---|:---|
| `GET` | `/health` | Check API health and LLM provider status |

---

## 📖 Example Usage

### 🚀 Advanced Analysis with LangChain

```bash
curl -X POST "http://localhost:8000/advanced/analyze-chain" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "La inteligencia artificial está transformando nuestra sociedad de manera significativa. Este ensayo analiza tres aspectos fundamentales de esta revolución tecnológica...",
    "criteria": ["originalidad", "profundidad", "evidencia"],
    "language": "es"
  }'
```

**Response:**
```json
{
  "overview": "La evaluación general del ensayo es positiva. El autor aborda tres aspectos cruciales de la transformación social impulsada por la inteligencia artificial...",
  "per_criterion": [
    {
      "etiqueta": "originalidad",
      "criterio": "¿Se usan enfoques creativos, metáforas o comparaciones inesperadas?",
      "valorMaximo": 22,
      "logro": "El ensayo utiliza una variedad de métodos creativos y no convencionales para explicar los impactos de la IA...",
      "evaluacion": "bueno",
      "puntuacion": 15
    }
  ],
  "total_score": 66,
  "max_possible_score": 100,
  "percentage": 66.0
}
```

### Standard Analysis

```bash
curl -X POST "http://localhost:8000/analyze/feedback" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Students should learn critical thinking...",
    "criteria": ["originalidad", "coherencia"],
    "language": "es"
  }'
```

### Get Section Guidance

```bash
curl -X POST "http://localhost:8000/guide" \
  -H "Content-Type: application/json" \
  -d '{
    "section": "claim",
    "language": "en"
  }'
```

---

## 🔍 LangChain vs Standard Analysis Comparison

### **Quality Comparison**

| Aspect | Standard Analysis | Advanced LangChain Analysis |
|--------|------------------|----------------------------|
| **Overview Length** | 1 sentence (25 words) | 4 paragraphs (150+ words) |
| **Detail Level** | Superficial | Deep and structured |
| **Strengths Identification** | Not mentioned | Specific strengths identified |
| **Improvement Suggestions** | Not mentioned | Concrete improvement suggestions |
| **Processing Method** | 1 LLM call | 8 LLM calls (1 overview + 7 criteria) |
| **Context Awareness** | Limited | Contextual accumulation |
| **Precision** | General | Specific per criterion |
| **Consistency** | Variable | High (Pydantic validation) |

### **Technical Benefits**

| Feature | Standard | LangChain Advanced |
|---------|----------|-------------------|
| **JSON Parsing** | Manual `safe_json_parse()` (227 lines) | `PydanticOutputParser` (automatic) |
| **Error Handling** | Manual JSON error handling | Automatic markdown cleaning + validation |
| **Type Safety** | Basic Pydantic validation | Advanced Pydantic with auto-conversion |
| **Extensibility** | Modify complex prompts | Add criteria to simple loop |
| **Debugging** | Difficult (JSON parsing issues) | Easy (structured logs) |
| **Testing** | Complex (manual parsing) | Simple (Pydantic validation) |

---

## ⚙️ LLM Provider Configuration

### Registry Pattern Architecture

The project uses a **Registry Pattern** for LLM providers, making it extremely easy to add new providers without modifying existing code.

#### Supported Providers

| Provider | Description | Setup | Adapter |
|:---------|:------------|:------|:--------|
| `ollama` | Local Ollama models (Llama, Mistral, etc.) | `ollama pull llama3.1 && ollama serve` | OllamaAdapter |
| `openai` | OpenAI API models (GPT-4, GPT-3.5) | Set `OPENAI_API_KEY` environment variable | OpenAIAdapter |
| `qwen2.5` | Qwen2.5 models via Ollama | `ollama pull qwen2.5 && ollama serve` | OllamaAdapter |
| `langchain` | **Advanced LangChain features** | Uses OpenAI/Ollama with LangChain | LangChainAdapter |

### 🚀 Adding New LLM Providers

Adding a new LLM provider is now **incredibly simple** - just 2 steps:

#### Step 1: Create Provider Adapter
Create `app/adapters/providers/your_provider.py`:

```python
from app.adapters.base import BaseLLMAdapter

class YourProviderAdapter(BaseLLMAdapter):
    def generate(self, prompt: str, **kwargs) -> str:
        # Your provider-specific implementation
        # Handle API calls, authentication, response parsing
        return response_text
```

#### Step 2: Register the Provider
Add 2 lines to `app/adapters/llm_adapter.py`:

```python
from app.adapters.providers.your_provider import YourProviderAdapter
LLMProviderRegistry.register("your_provider", YourProviderAdapter)
```

#### Step 3: Use It! 🎉
```bash
LLM_PROVIDER=your_provider
LLM_MODEL=your_model_name
```

**That's it!** No other files need to be modified.

### Configuration Examples

#### Development with Ollama
```bash
LLM_PROVIDER=ollama
LLM_MODEL=llama3.1
```

#### Development with Qwen2.5
```bash
LLM_PROVIDER=qwen2.5
LLM_MODEL=qwen2.5:3b
```

#### Production with OpenAI
```bash
LLM_PROVIDER=openai
LLM_MODEL=gpt-4o-mini
OPENAI_API_KEY=your_api_key_here
```

#### 🚀 Advanced Analysis with LangChain
```bash
LLM_PROVIDER=langchain
LLM_MODEL=gpt-4o-mini  # or llama3.1 for local
OPENAI_API_KEY=your_api_key_here  # if using OpenAI models
```

#### Mixed Configuration (Cost Optimization)
```bash
LLM_PROVIDER=langchain
LLM_MODEL=gpt-4o-mini
LLM_MODEL_AI_DETECTION=openai:gpt-4o          # Premium model for critical tasks
LLM_MODEL_FEEDBACK=openai:gpt-4o-mini         # Balanced model for feedback
LLM_MODEL_GUIDANCE=qwen2.5:qwen2.5:3b         # Free model for guidance
LLM_MODEL_SECTION_CHECK=qwen2.5:qwen2.5:3b    # Free model for section checks
```

---

## 🧪 Testing

The project includes comprehensive test coverage with 107+ tests.

### Run Tests

```bash
# All tests
pytest

# With verbose output
pytest -v

# With coverage report
pytest --cov=app --cov-report=html

# Specific test file
pytest tests/test_services_advanced_analyzer.py

# LangChain-specific tests
pytest tests/test_services_advanced_analyzer.py::test_analyze_with_chain
```

### Test Structure

- **Unit Tests**: Services, adapters, utilities
- **Integration Tests**: API endpoints
- **LangChain Tests**: Advanced analyzer functionality
- **Fixtures**: Mock LLM responses
- **Coverage**: ~90% code coverage

**Current Status:** ✅ 107+ tests passing

---

## 🎯 Architecture Highlights

### Clean Architecture
- **Routers** → **Services** → **Adapters** → **Models**
- Clear separation of concerns
- No circular dependencies
- Testable components

### 🚀 Advanced LangChain Integration
- **Multi-step Chains**: Sequential analysis with context accumulation
- **PydanticOutputParser**: Robust structured output validation
- **Context Awareness**: Each step builds upon previous analysis
- **Granular Processing**: Individual chains per evaluation criterion
- **Error Resilience**: Automatic JSON cleaning and validation

### Modular Prompt Generation
- Separated concerns (constants, languages, sections, templates)
- Reusable components
- Easy to extend and maintain

### Robust LLM Integration
- **Registry Pattern**: Easy to add new LLM providers
- **Unified Interface**: Single API for all providers
- **Multi-Provider Support**: Ollama, OpenAI, Qwen2.5, LangChain
- **Function-Specific Models**: Different models for different tasks
- **Intelligent Fallbacks**: Automatic failover to cheaper models

### Token Tracking & Cost Management
- SQLite-backed tracking
- Real-time cost calculations
- Comprehensive reporting
- CSV export capability

### Resilience & Reliability
- Circuit breakers prevent cascading failures
- Exponential backoff with jitter
- Error classification and retry logic
- Comprehensive error handling

---

## 🔧 Development Guide

### Adding a New LLM Provider

The Registry Pattern makes adding new LLM providers incredibly simple:

#### Example: Adding Claude (Anthropic)

1. **Create the adapter** (`app/adapters/providers/claude.py`):
```python
from app.adapters.base import BaseLLMAdapter
import requests

class ClaudeAdapter(BaseLLMAdapter):
    def generate(self, prompt: str, **kwargs) -> str:
        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/json"
        }
        payload = {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": kwargs.get("num_predict", 512)
        }
        
        response = requests.post(
            f"{self.base_url}/v1/messages",
            json=payload,
            headers=headers
        )
        response.raise_for_status()
        return response.json()["content"][0]["text"]
```

2. **Register the provider** (add to `app/adapters/llm_adapter.py`):
```python
from app.adapters.providers.claude import ClaudeAdapter
LLMProviderRegistry.register("claude", ClaudeAdapter)
```

3. **Use it**:
```bash
LLM_PROVIDER=claude
LLM_MODEL=claude-3-sonnet-20240229
```

**Total effort**: 1 new file + 2 lines of code! 🎉

---

## 🛠️ Technical Requirements

* **Python** ≥ 3.10
* **FastAPI** ≥ 0.115
* **Pydantic** ≥ 2.0
* **LangChain** ≥ 0.3.0 (for advanced features)
* **Pytest** for testing
* **(Optional)** Ollama for local development
  ```bash
  ollama pull llama3.1
  ollama serve
  ```
* **(Optional)** Qwen2.5 models via Ollama
  ```bash
  ollama pull qwen2.5
  ollama serve
  ```

---

## 🔐 Environment Variables

### Required
- `LLM_PROVIDER`: LLM provider (ollama/openai/qwen2.5/langchain)
- `LLM_MODEL`: Default model to use

### Optional
- `OPENAI_API_KEY`: OpenAI API key (required for production)
- `DEFAULT_LANGUAGE`: Default response language (en/es)
- `ENABLE_TOKEN_TRACKING`: Enable usage tracking (true/false)
- `TOKEN_TRACKING_DB_PATH`: Path to tracking database

### Function-Specific Models
- `LLM_MODEL_AI_DETECTION`: Model for AI detection (e.g., "openai:gpt-4o")
- `LLM_MODEL_FEEDBACK`: Model for essay feedback (e.g., "openai:gpt-4o-mini")
- `LLM_MODEL_GUIDANCE`: Model for section guidance (e.g., "qwen2.5:qwen2.5:3b")
- `LLM_MODEL_SECTION_CHECK`: Model for section checking (e.g., "qwen2.5:qwen2.5:3b")

### Fallback Configuration
- `LLM_FALLBACK_*`: Comma-separated fallback chains for each function
- `*_MAX_RETRIES`: Maximum retries per model type
- `CIRCUIT_BREAKER_*`: Circuit breaker settings

See `.env.example` for complete configuration options.

---

## 🗺️ Development Roadmap

| Phase | Goal | Status |
|:------|:-----|:-------|
| A | Base structure, models, routers, healthcheck | ✅ |
| B | Implement prompts and utilities | ✅ |
| C | LLM adapters (Ollama, OpenAI) | ✅ |
| D | Services: analyzer and guidance | ✅ |
| E | Unit & integration tests | ✅ |
| F | Token tracking and fallback system | ✅ |
| G | Modular refactoring (prompts, tracking, fallbacks) | ✅ |
| H | Registry Pattern implementation | ✅ |
| I | **LangChain Integration** | ✅ |
| J | Docker + CI/CD + deployment | 🔄 |

---

## 🚀 Future LangChain Opportunities

### 🧠 **Phase 1: Memory & Conversational Analysis**
- **Conversational Memory**: Track student progress across multiple essay submissions
- **Learning Path Analysis**: Understand student improvement patterns
- **Personalized Feedback**: Adapt recommendations based on student history
- **Session Continuity**: Maintain context across analysis sessions

### 🔍 **Phase 2: RAG (Retrieval Augmented Generation)**
- **Knowledge Base Integration**: Connect to educational databases
- **Citation Analysis**: Verify and suggest academic sources
- **Plagiarism Detection**: Compare against known academic works
- **Topic-Specific Guidance**: Provide domain-specific writing advice

### 🤖 **Phase 3: Agent-Based Workflows**
- **Writing Assistant Agent**: Multi-step essay improvement workflow
- **Research Agent**: Automated fact-checking and source validation
- **Style Analysis Agent**: Advanced writing style and tone analysis
- **Peer Review Agent**: Simulate peer review processes

### 📊 **Phase 4: Advanced Analytics**
- **Learning Analytics**: Track student writing development over time
- **Classroom Insights**: Aggregate analysis for teachers
- **Writing Pattern Recognition**: Identify common student challenges
- **Predictive Feedback**: Anticipate areas needing improvement

### 🔗 **Phase 5: Integration Ecosystem**
- **LMS Integration**: Connect with Learning Management Systems
- **Google Classroom**: Direct integration with Google Workspace
- **Canvas/Moodle**: Support for popular educational platforms
- **API Ecosystem**: Third-party integrations and plugins

---

## 📊 Features in Detail

### 🚀 LangChain Advanced Features
- **Multi-step Analysis**: Sequential processing with context accumulation
- **PydanticOutputParser**: Automatic structured output validation
- **Context Awareness**: Each analysis step builds upon previous results
- **Granular Evaluation**: Individual chains for each Toulmin criterion
- **Error Resilience**: Automatic JSON cleaning and type conversion

### Token Tracking System
- Automatic token counting for all LLM calls
- Cost calculation based on provider and model
- SQLite storage for persistent tracking
- Query by date range, function, or provider
- Export data for external analysis

### Fallback System
- Automatic model degradation on failure
- Configurable fallback chains per function
- Retry logic with exponential backoff
- Circuit breakers for failing models
- Error classification (retryable vs non-retryable)

### Cost Optimization
- Function-specific model selection
- Use cheaper models for simpler tasks
- Reserve premium models for critical functions
- Daily/monthly cost reporting
- Usage trends analysis

---

## 🎓 Educational Impact

### For Students
- **Detailed Feedback**: Comprehensive analysis using Extended Toulmin Model
- **Learning Progression**: Track improvement over time
- **Personalized Guidance**: Adapt to individual writing styles
- **Academic Standards**: Align with university-level evaluation criteria

### For Educators
- **Consistent Evaluation**: Standardized criteria across all essays
- **Time Efficiency**: Automated analysis frees time for teaching
- **Classroom Insights**: Aggregate data for curriculum improvement
- **Scalable Assessment**: Handle large class sizes efficiently

### For Institutions
- **Quality Assurance**: Maintain academic standards
- **Resource Optimization**: Reduce manual grading workload
- **Data-Driven Decisions**: Analytics for curriculum development
- **Technology Integration**: Modern AI-powered educational tools

---

## 👨‍💻 Author

**Fernando Herrera Sánchez**  
Software Engineer 💡  
📍 México

**GitHub**: [@fernando-herrera](https://github.com/fernandohs)  
**LinkedIn**: [Fernando Herrera](https://www.linkedin.com/in/fernando-herrera-sanchez-90699859/)

---

## 📄 License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## 🤝 Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

### 🚀 LangChain Contributions
We especially welcome contributions related to:
- New LangChain features and integrations
- Advanced analysis workflows
- Memory and RAG implementations
- Agent-based systems
- Educational AI applications

---

## ⭐ Star History

If you find this project useful, please consider giving it a star! ⭐

---

## 🔗 Related Projects

- [LangChain Documentation](https://python.langchain.com/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Extended Toulmin Model](https://en.wikipedia.org/wiki/Toulmin_model)
- [Ollama Models](https://ollama.ai/)

---

## 📈 Project Metrics

- **Lines of Code**: 2,500+
- **Test Coverage**: 90%+
- **API Endpoints**: 15+
- **LLM Providers**: 4+ (Ollama, OpenAI, Qwen2.5, LangChain)
- **Supported Languages**: 2 (English, Spanish)
- **Evaluation Criteria**: 7 (Extended Toulmin Model)
- **Test Cases**: 107+

---

## 🏆 Achievements

- ✅ **Production Ready**: Enterprise-grade features and reliability
- ✅ **LangChain Integration**: Advanced AI workflows and chains
- ✅ **Comprehensive Testing**: 90%+ test coverage
- ✅ **Modular Architecture**: Clean, maintainable, extensible code
- ✅ **Multi-Provider Support**: Flexible LLM provider system
- ✅ **Educational Focus**: Purpose-built for academic essay analysis
- ✅ **Open Source**: MIT licensed for educational use