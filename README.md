# ğŸ§  Essay Mentor API

> **FastAPI + Modular Architecture + LLM Adapters (Ollama / OpenAI) with Token Tracking & Fallback System**

A production-ready FastAPI service designed to **analyze and guide** the writing of short student essays (max. 8,000 characters). Built with clean architecture principles, comprehensive testing, and enterprise-grade features.

The service provides automated tools to:

* **Estimate** whether a text may have been generated by AI.
* Offer **personalized feedback** based on multiple criteria (originality, creativity, coherence, etc.).
* **Guide** students with section-specific explanations and writing tips for argumentative essays based on the **Toulmin model**:
    * *Claim*
    * *Reasoning*
    * *Evidence*
    * *Backing*
    * *Reservation*
    * *Rebuttal*

---

## âœ¨ Key Features

### Core Features
* **Modular architecture** with clear separation of concerns (routers, services, adapters, prompts, models, utils)
* **Interchangeable LLM adapters** (Ollama for local development, OpenAI for production)
* **Swagger-ready endpoints** with comprehensive API documentation
* **Configurable prompts** with educational rules (never generate the student's answer)
* **Structured feedback** based on customizable evaluation criteria
* **Bilingual-ready**: Configure default language via `DEFAULT_LANGUAGE` env variable (en/es)

### Enterprise Features
* **Token usage tracking** with SQLite database
* **Cost tracking** for OpenAI usage with daily/monthly reports
* **Intelligent fallback system** for OpenAI models (gpt-4o â†’ gpt-4o-mini â†’ gpt-3.5-turbo)
* **Retry logic** with exponential backoff and jitter
* **Circuit breakers** to prevent cascading failures
* **Function-specific model selection** for cost optimization
* **Usage analytics** API with CSV export capability

---

## ğŸ—ï¸ Project Structure

```
EssayMentorApi/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                           # FastAPI application entry point
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ config.py                     # Configuration management
â”‚   â”œâ”€â”€ adapters/
â”‚   â”‚   â”œâ”€â”€ base.py                       # Base LLM adapter interface
â”‚   â”‚   â”œâ”€â”€ registry.py                   # Provider registry system
â”‚   â”‚   â”œâ”€â”€ llm_adapter.py                # Unified LLM adapter wrapper
â”‚   â”‚   â””â”€â”€ providers/                    # Provider-specific adapters
â”‚   â”‚       â”œâ”€â”€ ollama.py                 # Ollama adapter (local models)
â”‚   â”‚       â””â”€â”€ openai.py                 # OpenAI adapter (API models)
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ types.py                      # Section enums and mappings
â”‚   â”‚   â”œâ”€â”€ essay.py                      # Essay request/response models
â”‚   â”‚   â”œâ”€â”€ analyze.py                    # AI detection & feedback models
â”‚   â”‚   â”œâ”€â”€ guide.py                      # Guidance models
â”‚   â”‚   â””â”€â”€ usage.py                      # Token usage tracking models
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â”œâ”€â”€ factory.py                    # Prompt generation wrapper
â”‚   â”‚   â”œâ”€â”€ generators/                   # Modular prompt components
â”‚   â”‚   â”‚   â”œâ”€â”€ constants.py             # Educational rules & constants
â”‚   â”‚   â”‚   â”œâ”€â”€ language_handler.py      # Multi-language support
â”‚   â”‚   â”‚   â”œâ”€â”€ section_definitions.py   # Toulmin model sections
â”‚   â”‚   â”‚   â”œâ”€â”€ templates.py             # Prompt templates
â”‚   â”‚   â”‚   â””â”€â”€ __init__.py              # Main generator class
â”‚   â”‚   â””â”€â”€ criteria_data.py             # Evaluation criteria
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ analyzer.py                   # AI detection & feedback logic
â”‚   â”‚   â””â”€â”€ guidance.py                   # Essay section guidance logic
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ meta.py                       # Health & metadata endpoints
â”‚   â”‚   â”œâ”€â”€ analyze.py                    # Analysis endpoints
â”‚   â”‚   â”œâ”€â”€ guide.py                      # Guidance endpoints
â”‚   â”‚   â””â”€â”€ usage.py                      # Token usage & reporting endpoints
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ json_parse.py                 # Robust JSON parsing
â”‚       â”œâ”€â”€ criteria.py                   # Criteria validation
â”‚       â”œâ”€â”€ text_format.py                # Text formatting utilities
â”‚       â”œâ”€â”€ token_tracker.py              # Token tracking wrapper
â”‚       â”œâ”€â”€ fallback_manager.py           # Fallback system wrapper
â”‚       â”œâ”€â”€ tracking/                     # Token tracking components
â”‚       â”‚   â”œâ”€â”€ database_manager.py      # SQLite operations
â”‚       â”‚   â”œâ”€â”€ cost_calculator.py       # Cost calculations
â”‚       â”‚   â”œâ”€â”€ report_generator.py      # Usage reports
â”‚       â”‚   â””â”€â”€ __init__.py              # Main tracker class
â”‚       â””â”€â”€ fallback/                     # Fallback system components
â”‚           â”œâ”€â”€ circuit_breaker.py       # Circuit breaker logic
â”‚           â”œâ”€â”€ retry_logic.py           # Retry strategies
â”‚           â”œâ”€â”€ error_classifier.py      # Error classification
â”‚           â””â”€â”€ __init__.py              # Main fallback manager
â”œâ”€â”€ tests/                                # Comprehensive test suite
â”‚   â”œâ”€â”€ conftest.py                      # Shared fixtures
â”‚   â”œâ”€â”€ test_adapters_llm.py            # Adapter tests
â”‚   â”œâ”€â”€ test_services_analyzer.py       # Analyzer tests
â”‚   â”œâ”€â”€ test_services_guidance.py       # Guidance tests
â”‚   â”œâ”€â”€ test_utils_json_parse.py        # Utils tests
â”‚   â”œâ”€â”€ test_utils_criteria.py          # Criteria tests
â”‚   â”œâ”€â”€ test_utils_text_format.py       # Text format tests
â”‚   â””â”€â”€ test_routers_integration.py     # Integration tests
â”œâ”€â”€ examples/                             # Usage examples
â”œâ”€â”€ requirements.txt                      # Python dependencies
â”œâ”€â”€ .env.example                          # Environment variables template
â””â”€â”€ README.md                             # This file
```

---

## ğŸš€ Quick Start

### 1. Clone the repository

```bash
git clone https://github.com/<your-username>/EssayMentorApi.git
cd EssayMentorApi
```

### 2. Create virtual environment

```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Configure environment variables

Create a `.env` file based on `.env.example`:

```bash
# Application Configuration
APP_NAME=Essay Mentor API
APP_VERSION=1.0.0
HOST=0.0.0.0
PORT=8000
CORS_ORIGINS=*

# LLM Configuration
LLM_PROVIDER=ollama  # or 'openai' for production, 'qwen2.5' for Qwen models
LLM_MODEL=llama3.1   # Default model
OLLAMA_URL=http://localhost:11434

# OpenAI Configuration (for production)
OPENAI_API_KEY=your_key_here
OPENAI_BASE_URL=https://api.openai.com/v1

# Function-specific model selection (cost optimization)
LLM_MODEL_AI_DETECTION=   # Empty = use LLM_MODEL
LLM_MODEL_FEEDBACK=        # Empty = use LLM_MODEL
LLM_MODEL_GUIDANCE=        # Empty = use LLM_MODEL
LLM_MODEL_SECTION_CHECK=   # Empty = use LLM_MODEL

# Fallback Configuration (OpenAI models only)
LLM_FALLBACK_AI_DETECTION=gpt-4o,gpt-4o-mini,gpt-3.5-turbo
LLM_FALLBACK_FEEDBACK=gpt-4o,gpt-4o-mini,gpt-3.5-turbo
LLM_FALLBACK_GUIDANCE=gpt-4o-mini,gpt-3.5-turbo
LLM_FALLBACK_SECTION_CHECK=gpt-4o-mini,gpt-3.5-turbo

# Retry Configuration
GPT4O_MAX_RETRIES=2
GPT4O_MINI_MAX_RETRIES=3
GPT35_TURBO_MAX_RETRIES=5

# Circuit Breaker Configuration
CIRCUIT_BREAKER_FAILURE_THRESHOLD=5
CIRCUIT_BREAKER_RECOVERY_TIMEOUT=300

# Language Configuration
DEFAULT_LANGUAGE=en  # en=English, es=Spanish

# Token Tracking Configuration
ENABLE_TOKEN_TRACKING=true
TOKEN_TRACKING_DB_PATH=usage_tracking.db
```

### 5. Run the API

```bash
# Development
uvicorn app.main:app --reload

# Production
gunicorn app.main:app -w 4 -k uvicorn.workers.UvicornWorker
```

**Access Points:**
* **Swagger UI:** `http://localhost:8000/docs`
* **ReDoc:** `http://localhost:8000/redoc`
* **OpenAPI JSON:** `http://localhost:8000/openapi.json`

---

## ğŸ“š API Endpoints

### Analysis Endpoints

| Method | Route | Description |
|:---|:---|:---|
| `POST` | `/analyze/ai-likelihood` | Estimate AI-generated likelihood (0-100 score) |
| `POST` | `/analyze/feedback` | Generate structured writing feedback with criteria evaluation |

### Guidance Endpoints

| Method | Route | Description |
|:---|:---|:---|
| `POST` | `/guide` | Provide guidance for essay sections (claim, reasoning, etc.) |
| `POST` | `/guide/check-section` | Review a specific section and suggest improvements |

### Usage & Reporting Endpoints

| Method | Route | Description |
|:---|:---|:---|
| `GET` | `/admin/usage/daily` | Get daily token usage report |
| `GET` | `/admin/usage/report` | Get usage report for date range |
| `GET` | `/admin/usage/costs` | Get cost summary |
| `GET` | `/admin/usage/trends` | Get usage trends |
| `GET` | `/admin/usage/export/csv` | Export usage data as CSV |

### Meta Endpoints

| Method | Route | Description |
|:---|:---|:---|
| `GET` | `/health` | Check API health and LLM provider status |

---

## ğŸ“– Example Usage

### Analyze AI Likelihood

```bash
curl -X POST "http://localhost:8000/analyze/ai-likelihood" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "This essay argues that technology enhances education...",
    "language": "en"
  }'
```

**Response:**
```json
{
  "score": 35,
  "rationale": "The text shows natural variations in style...",
  "caveats": ["Analysis is probabilistic", "Context matters"]
}
```

### Get Essay Feedback

```bash
curl -X POST "http://localhost:8000/analyze/feedback" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Students should learn critical thinking...",
    "criteria": ["originalidad", "coherencia"],
    "language": "es"
  }'
```

### Get Section Guidance

```bash
curl -X POST "http://localhost:8000/guide" \
  -H "Content-Type: application/json" \
  -d '{
    "section": "claim",
    "language": "en"
  }'
```

### Check Usage Statistics

```bash
# Daily usage
curl "http://localhost:8000/admin/usage/daily"

# Cost summary (last 7 days)
curl "http://localhost:8000/admin/usage/costs?days=7"

# Export to CSV
curl "http://localhost:8000/admin/usage/export/csv?start_date=2024-01-01&end_date=2024-01-31"
```

---

## âš™ï¸ LLM Provider Configuration

### Registry Pattern Architecture

The project uses a **Registry Pattern** for LLM providers, making it extremely easy to add new providers without modifying existing code.

#### Supported Providers

| Provider | Description | Setup | Adapter |
|:---------|:------------|:------|:--------|
| `ollama` | Local Ollama models (Llama, Mistral, etc.) | `ollama pull llama3.1 && ollama serve` | OllamaAdapter |
| `openai` | OpenAI API models (GPT-4, GPT-3.5) | Set `OPENAI_API_KEY` environment variable | OpenAIAdapter |
| `qwen2.5` | Qwen2.5 models via Ollama | `ollama pull qwen2.5 && ollama serve` | OllamaAdapter |

### ğŸš€ Adding New LLM Providers

Adding a new LLM provider is now **incredibly simple** - just 2 steps:

#### Step 1: Create Provider Adapter
Create `app/adapters/providers/your_provider.py`:

```python
from app.adapters.base import BaseLLMAdapter

class YourProviderAdapter(BaseLLMAdapter):
    def generate(self, prompt: str, **kwargs) -> str:
        # Your provider-specific implementation
        # Handle API calls, authentication, response parsing
        return response_text
```

#### Step 2: Register the Provider
Add 2 lines to `app/adapters/llm_adapter.py`:

```python
from app.adapters.providers.your_provider import YourProviderAdapter
LLMProviderRegistry.register("your_provider", YourProviderAdapter)
```

#### Step 3: Use It! ğŸ‰
```bash
LLM_PROVIDER=your_provider
LLM_MODEL=your_model_name
```

**That's it!** No other files need to be modified.

### Configuration Examples

#### Development with Ollama
```bash
LLM_PROVIDER=ollama
LLM_MODEL=llama3.1
```

#### Development with Qwen2.5
```bash
LLM_PROVIDER=qwen2.5
LLM_MODEL=qwen2.5:3b
```

#### Production with OpenAI
```bash
LLM_PROVIDER=openai
LLM_MODEL=gpt-4o-mini
OPENAI_API_KEY=your_api_key_here
```

#### Mixed Configuration (Cost Optimization)
```bash
LLM_PROVIDER=openai
LLM_MODEL=gpt-4o-mini
LLM_MODEL_AI_DETECTION=openai:gpt-4o          # Premium model for critical tasks
LLM_MODEL_FEEDBACK=openai:gpt-4o-mini         # Balanced model for feedback
LLM_MODEL_GUIDANCE=qwen2.5:qwen2.5:3b         # Free model for guidance
LLM_MODEL_SECTION_CHECK=qwen2.5:qwen2.5:3b    # Free model for section checks
```

### ğŸ”„ Fallback System

The system includes a sophisticated fallback mechanism for OpenAI models:

#### How It Works
1. **Primary Model**: Attempts to use the configured model
2. **Fallback Chain**: If primary fails, tries cheaper models in sequence
3. **Circuit Breaker**: Temporarily stops requests to failing models
4. **Retry Logic**: Exponential backoff with jitter for transient failures

#### Fallback Configuration
```bash
# Define fallback chains for each function
LLM_FALLBACK_AI_DETECTION=gpt-4o,gpt-4o-mini,gpt-3.5-turbo
LLM_FALLBACK_FEEDBACK=gpt-4o,gpt-4o-mini,gpt-3.5-turbo
LLM_FALLBACK_GUIDANCE=gpt-4o-mini,gpt-3.5-turbo
LLM_FALLBACK_SECTION_CHECK=gpt-4o-mini,gpt-3.5-turbo

# Retry settings
GPT4O_MAX_RETRIES=2
GPT4O_MINI_MAX_RETRIES=3
GPT35_TURBO_MAX_RETRIES=5

# Circuit breaker settings
CIRCUIT_BREAKER_FAILURE_THRESHOLD=5
CIRCUIT_BREAKER_RECOVERY_TIMEOUT=300  # 5 minutes
```

#### Fallback Benefits
- âœ… **Cost Optimization**: Automatically uses cheaper models when possible
- âœ… **Reliability**: Continues working even if premium models fail
- âœ… **Performance**: Circuit breakers prevent cascading failures
- âœ… **Monitoring**: Full tracking of fallback usage and success rates

---

## ğŸ§ª Testing

The project includes comprehensive test coverage with 107 tests.

### Run Tests

```bash
# All tests
pytest

# With verbose output
pytest -v

# With coverage report
pytest --cov=app --cov-report=html

# Specific test file
pytest tests/test_services_analyzer.py

# Specific test
pytest tests/test_services_analyzer.py::test_analyze_ai_likelihood_success
```

### Test Structure

- **Unit Tests**: Services, adapters, utilities
- **Integration Tests**: API endpoints
- **Fixtures**: Mock LLM responses
- **Coverage**: ~90% code coverage

**Current Status:** âœ… 107 tests passing

---

## ğŸ¯ Architecture Highlights

### Clean Architecture
- **Routers** â†’ **Services** â†’ **Adapters** â†’ **Models**
- Clear separation of concerns
- No circular dependencies
- Testable components

### Modular Prompt Generation
- Separated concerns (constants, languages, sections, templates)
- Reusable components
- Easy to extend and maintain

### Robust LLM Integration
- **Registry Pattern**: Easy to add new LLM providers
- **Unified Interface**: Single API for all providers
- **Multi-Provider Support**: Ollama, OpenAI, Qwen2.5, and more
- **Function-Specific Models**: Different models for different tasks
- **Intelligent Fallbacks**: Automatic failover to cheaper models

### Token Tracking & Cost Management
- SQLite-backed tracking
- Real-time cost calculations
- Comprehensive reporting
- CSV export capability

### Resilience & Reliability
- Circuit breakers prevent cascading failures
- Exponential backoff with jitter
- Error classification and retry logic
- Comprehensive error handling

### Registry Pattern Benefits
- **Extensibility**: Add new LLM providers without modifying existing code
- **Maintainability**: Each provider is isolated and independently testable
- **Scalability**: Easy to support multiple providers and aliases
- **Backward Compatibility**: Existing code continues to work unchanged

---

## ğŸ”§ Development Guide

### Adding a New LLM Provider

The Registry Pattern makes adding new LLM providers incredibly simple:

#### Example: Adding Claude (Anthropic)

1. **Create the adapter** (`app/adapters/providers/claude.py`):
```python
from app.adapters.base import BaseLLMAdapter
import requests

class ClaudeAdapter(BaseLLMAdapter):
    def generate(self, prompt: str, **kwargs) -> str:
        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/json"
        }
        payload = {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": kwargs.get("num_predict", 512)
        }
        
        response = requests.post(
            f"{self.base_url}/v1/messages",
            json=payload,
            headers=headers
        )
        response.raise_for_status()
        return response.json()["content"][0]["text"]
```

2. **Register the provider** (add to `app/adapters/llm_adapter.py`):
```python
from app.adapters.providers.claude import ClaudeAdapter
LLMProviderRegistry.register("claude", ClaudeAdapter)
```

3. **Use it**:
```bash
LLM_PROVIDER=claude
LLM_MODEL=claude-3-sonnet-20240229
```

**Total effort**: 1 new file + 2 lines of code! ğŸ‰

### Provider Aliases

You can create aliases for providers:

```python
# Register aliases
LLMProviderRegistry.register_alias("gpt4", "openai")
LLMProviderRegistry.register_alias("llama", "ollama")

# Use aliases
adapter = get_llm_adapter(provider="gpt4", model="gpt-4o")
```

---

## ğŸ› ï¸ Technical Requirements

* **Python** â‰¥ 3.10
* **FastAPI** â‰¥ 0.115
* **Pydantic** â‰¥ 2.0
* **Pytest** for testing
* **(Optional)** Ollama for local development
  ```bash
  ollama pull llama3.1
  ollama serve
  ```
* **(Optional)** Qwen2.5 models via Ollama
  ```bash
  ollama pull qwen2.5
  ollama serve
  ```

---

## ğŸ” Environment Variables

### Required
- `LLM_PROVIDER`: LLM provider (ollama/openai/qwen2.5)
- `LLM_MODEL`: Default model to use

### Optional
- `OPENAI_API_KEY`: OpenAI API key (required for production)
- `DEFAULT_LANGUAGE`: Default response language (en/es)
- `ENABLE_TOKEN_TRACKING`: Enable usage tracking (true/false)
- `TOKEN_TRACKING_DB_PATH`: Path to tracking database

### Function-Specific Models
- `LLM_MODEL_AI_DETECTION`: Model for AI detection (e.g., "openai:gpt-4o")
- `LLM_MODEL_FEEDBACK`: Model for essay feedback (e.g., "openai:gpt-4o-mini")
- `LLM_MODEL_GUIDANCE`: Model for section guidance (e.g., "qwen2.5:qwen2.5:3b")
- `LLM_MODEL_SECTION_CHECK`: Model for section checking (e.g., "qwen2.5:qwen2.5:3b")

### Fallback Configuration
- `LLM_FALLBACK_*`: Comma-separated fallback chains for each function
- `*_MAX_RETRIES`: Maximum retries per model type
- `CIRCUIT_BREAKER_*`: Circuit breaker settings

See `.env.example` for complete configuration options.

---

## ğŸ—ºï¸ Development Roadmap

| Phase | Goal | Status |
|:------|:-----|:-------|
| A | Base structure, models, routers, healthcheck | âœ… |
| B | Implement prompts and utilities | âœ… |
| C | LLM adapters (Ollama, OpenAI) | âœ… |
| D | Services: analyzer and guidance | âœ… |
| E | Unit & integration tests | âœ… |
| F | Token tracking and fallback system | âœ… |
| G | Modular refactoring (prompts, tracking, fallbacks) | âœ… |
| H | Registry Pattern implementation | âœ… |
| I | Docker + CI/CD + deployment | ğŸ”„ |

---

## ğŸ“Š Features in Detail

### Token Tracking System
- Automatic token counting for all LLM calls
- Cost calculation based on provider and model
- SQLite storage for persistent tracking
- Query by date range, function, or provider
- Export data for external analysis

### Fallback System
- Automatic model degradation on failure
- Configurable fallback chains per function
- Retry logic with exponential backoff
- Circuit breakers for failing models
- Error classification (retryable vs non-retryable)

### Cost Optimization
- Function-specific model selection
- Use cheaper models for simpler tasks
- Reserve premium models for critical functions
- Daily/monthly cost reporting
- Usage trends analysis

---

## ğŸ‘¨â€ğŸ’» Author

**Fernando Herrera SÃ¡nchez**  
Software Engineer ğŸ’¡  
ğŸ“ MÃ©xico

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## â­ Star History

If you find this project useful, please consider giving it a star! â­
